{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse.linalg import svds\n",
    "\n",
    "import itertools\n",
    "import pickle\n",
    "import math\n",
    "import re\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import word2vec, Word2Vec\n",
    "import logging\n",
    "\n",
    "from word2vec_as_MF import Word2vecMF\n",
    "from functions import *\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read and preprocess enwik9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "data = []\n",
    "with open('data/xaa') as file:\n",
    "# with open('data/xaa') as file:\n",
    "    for line in file:\n",
    "        data+= [line[:-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wiki_to_wordlist(sentence, remove_stopwords=False ):\n",
    "        # Function to convert a document to a sequence of words,\n",
    "    # optionally removing stop words.  Returns a list of words.\n",
    "\n",
    "    # 3. Convert words to lower case and split them\n",
    "    words = sentence.split()\n",
    "    #\n",
    "    # 4. Optionally remove stop words (false by default)\n",
    "    \n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w in stops]\n",
    "    #\n",
    "    # 5. Return a list of words\n",
    "    return(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing sentences from training set\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "# sentences = []  # Initialize an empty list of sentences\n",
    "\n",
    "print(\"Parsing sentences from training set\")\n",
    "\n",
    "# sentences += [wiki_to_wordlist(sentence)]\n",
    "sentences = [sentence.split() for sentence in data]\n",
    "indices = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "for i, sentence in enumerate(sentences):\n",
    "    if not sentence:\n",
    "        pass\n",
    "    else:\n",
    "        indices.append(i)\n",
    "\n",
    "real_sentences = np.array(sentences)[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"print('real_sentences', real_sentences[-30:])\\nprint('indices', indices[-30:])\\n# print('sentences', sentences)\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''print('real_sentences', real_sentences[-30:])\n",
    "print('indices', indices[-30:])\n",
    "# print('sentences', sentences)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create word2vec as matrix factorization model\n",
    "dimension = 100\n",
    "model_enwik = Word2vecMF()\n",
    "model_enwik.data_to_matrices(real_sentences, dimension, 5, 'enwik-200/matrices.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# If the model has been already created, load it from file\n",
    "model_enwik = Word2vecMF()\n",
    "model_enwik.load_matrices(from_file='enwik-200/matrices.npz')'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5min, sys: 1.65 s, total: 5min 1s\n",
      "Wall time: 1min 57s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "skip = Word2Vec(real_sentences, size = dimension, compute_loss=True, min_count= 5, iter=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33622460.0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skip.get_latest_training_loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train ro_sgns model starting from SVD of SPPMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yerong/local/Anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:2: RuntimeWarning: divide by zero encountered in log\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# SVD initialization\n",
    "SPPMI = np.maximum(np.log(model_enwik.D) - np.log(model_enwik.B), 0)\n",
    "# print SPPMI\n",
    "u, s, vt = svds(SPPMI, k=dimension)\n",
    "C_svd = u.dot(np.sqrt(np.diag(s))).T\n",
    "W_svd = np.sqrt(np.diag(s)).dot(vt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_enwik.C = C_svd\n",
    "model_enwik.W = W_svd\n",
    "\n",
    "model_enwik.save_CW('enwik-200/initializations/SVD_dim200', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''# Train the model\n",
    "start_time = time.time()\n",
    "opt_experiment(model_enwik,\n",
    "               mode='PS', \n",
    "               d=dimension,\n",
    "               eta = 5e-5,\n",
    "               MAX_ITER=10,\n",
    "               from_iter=0,\n",
    "               start_from='SVD',\n",
    "               init=(True, C_svd, W_svd), display=True)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enwik-200/AMiter_fromSVD_dim100_step1e-05_factors\n",
      "Iter #: 119100 loss 953300.610762\n",
      "Iter #: 119200 loss 953296.7422\n",
      "Iter #: 119300 loss 953292.88041\n",
      "Iter #: 119400 loss 953289.025367\n",
      "Iter #: 119500 loss 953285.177045\n",
      "Iter #: 119600 loss 953281.335421\n",
      "Iter #: 119700 loss 953277.500468\n",
      "Iter #: 119800 loss 953273.672161\n",
      "Iter #: 119900 loss 953269.850477\n",
      "Iter #: 120000 loss 953266.035391\n",
      "Iter #: 120100 loss 953262.226879\n",
      "Iter #: 120200 loss 953258.424917\n",
      "Iter #: 120300 loss 953254.629482\n",
      "Iter #: 120400 loss 953250.84055\n",
      "Iter #: 120500 loss 953247.058097\n",
      "Iter #: 120600 loss 953243.282101\n",
      "Iter #: 120700 loss 953239.512538\n",
      "Iter #: 120800 loss 953235.749385\n",
      "Iter #: 120900 loss 953231.992619\n",
      "Iter #: 121000 loss 953228.242218\n",
      "Iter #: 121100 loss 953224.49816\n",
      "Iter #: 121200 loss 953220.760422\n",
      "Iter #: 121300 loss 953217.028982\n",
      "Iter #: 121400 loss 953213.30382\n",
      "Iter #: 121500 loss 953209.584912\n",
      "Iter #: 121600 loss 953205.872236\n",
      "Iter #: 121700 loss 953202.165772\n",
      "Iter #: 121800 loss 953198.465498\n",
      "Iter #: 121900 loss 953194.771393\n",
      "Iter #: 122000 loss 953191.083438\n",
      "Iter #: 122100 loss 953187.401608\n",
      "Iter #: 122200 loss 953183.725884\n",
      "Iter #: 122300 loss 953180.056248\n",
      "Iter #: 122400 loss 953176.392675\n",
      "Iter #: 122500 loss 953172.735148\n",
      "Iter #: 122600 loss 953169.083645\n",
      "Iter #: 122700 loss 953165.438147\n",
      "Iter #: 122800 loss 953161.798634\n",
      "Iter #: 122900 loss 953158.165086\n",
      "Iter #: 123000 loss 953154.53749\n",
      "Iter #: 123100 loss 953150.915813\n",
      "Iter #: 123200 loss 953147.300043\n",
      "Iter #: 123300 loss 953143.69016\n",
      "Iter #: 123400 loss 953140.086144\n",
      "Iter #: 123500 loss 953136.487979\n",
      "Iter #: 123600 loss 953132.895651\n",
      "Iter #: 123700 loss 953129.309126\n",
      "Iter #: 123800 loss 953125.728402\n",
      "Iter #: 123900 loss 953122.153444\n",
      "Iter #: 124000 loss 953118.584243\n",
      "Iter #: 124100 loss 953115.020777\n",
      "Iter #: 124200 loss 953111.463037\n",
      "Iter #: 124300 loss 953107.910991\n",
      "Iter #: 124400 loss 953104.364628\n",
      "Iter #: 124500 loss 953100.82393\n",
      "Iter #: 124600 loss 953097.288882\n",
      "Iter #: 124700 loss 953093.75946\n",
      "Iter #: 124800 loss 953090.23565\n",
      "Iter #: 124900 loss 953086.717435\n",
      "Iter #: 125000 loss 953083.204796\n",
      "Iter #: 125100 loss 953079.69772\n",
      "Iter #: 125200 loss 953076.196192\n",
      "Iter #: 125300 loss 953072.700181\n",
      "Iter #: 125400 loss 953069.20968\n",
      "Iter #: 125500 loss 953065.72467\n",
      "Iter #: 125600 loss 953062.245134\n",
      "Iter #: 125700 loss 953058.771062\n",
      "Iter #: 125800 loss 953055.302426\n",
      "Iter #: 125900 loss 953051.839215\n",
      "Iter #: 126000 loss 953048.381413\n",
      "Iter #: 126100 loss 953044.929004\n",
      "Iter #: 126200 loss 953041.481971\n",
      "Iter #: 126300 loss 953038.040297\n",
      "Iter #: 126400 loss 953034.60397\n",
      "Iter #: 126500 loss 953031.172968\n",
      "Iter #: 126600 loss 953027.747278\n",
      "Iter #: 126700 loss 953024.326885\n",
      "Iter #: 126800 loss 953020.911774\n",
      "Iter #: 126900 loss 953017.501926\n",
      "Iter #: 127000 loss 953014.097328\n",
      "Iter #: 127100 loss 953010.697964\n",
      "Iter #: 127200 loss 953007.303818\n",
      "Iter #: 127300 loss 953003.91488\n",
      "Iter #: 127400 loss 953000.531131\n",
      "Iter #: 127500 loss 952997.152551\n",
      "Iter #: 127600 loss 952993.779128\n",
      "Iter #: 127700 loss 952990.410851\n",
      "Iter #: 127800 loss 952987.0477\n",
      "Iter #: 127900 loss 952983.689663\n",
      "Iter #: 128000 loss 952980.336724\n",
      "Iter #: 128100 loss 952976.988871\n",
      "Iter #: 128200 loss 952973.646085\n",
      "Iter #: 128300 loss 952970.308355\n",
      "Iter #: 128400 loss 952966.975667\n",
      "Iter #: 128500 loss 952963.648004\n",
      "Iter #: 128600 loss 952960.325354\n",
      "Iter #: 128700 loss 952957.007701\n",
      "Iter #: 128800 loss 952953.695033\n",
      "Iter #: 128900 loss 952950.387333\n",
      "Iter #: 129000 loss 952947.084588\n",
      "Iter #: 129100 loss 952943.786789\n",
      "Iter #: 129200 loss 952940.493914\n",
      "Iter #: 129300 loss 952937.205957\n",
      "Iter #: 129400 loss 952933.922895\n",
      "Iter #: 129500 loss 952930.64472\n",
      "Iter #: 129600 loss 952927.371419\n",
      "Iter #: 129700 loss 952924.102976\n",
      "Iter #: 129800 loss 952920.839381\n",
      "Iter #: 129900 loss 952917.58062\n",
      "Iter #: 130000 loss 952914.326677\n",
      "Iter #: 130100 loss 952911.077544\n",
      "Iter #: 130200 loss 952907.833202\n",
      "Iter #: 130300 loss 952904.59364\n",
      "Iter #: 130400 loss 952901.358843\n",
      "Iter #: 130500 loss 952898.128799\n",
      "Iter #: 130600 loss 952894.903494\n",
      "Iter #: 130700 loss 952891.682919\n",
      "Iter #: 130800 loss 952888.467058\n",
      "Iter #: 130900 loss 952885.255898\n",
      "Iter #: 131000 loss 952882.04943\n",
      "Iter #: 131100 loss 952878.847638\n",
      "Iter #: 131200 loss 952875.650509\n",
      "Iter #: 131300 loss 952872.458052\n",
      "Iter #: 131400 loss 952869.270215\n",
      "Iter #: 131500 loss 952866.087005\n",
      "Iter #: 131600 loss 952862.908409\n",
      "Iter #: 131700 loss 952859.734417\n",
      "Iter #: 131800 loss 952856.565014\n",
      "Iter #: 131900 loss 952853.400188\n",
      "Iter #: 132000 loss 952850.239928\n",
      "Iter #: 132100 loss 952847.084221\n",
      "Iter #: 132200 loss 952843.933056\n",
      "Iter #: 132300 loss 952840.786421\n",
      "Iter #: 132400 loss 952837.644305\n",
      "Iter #: 132500 loss 952834.506693\n",
      "Iter #: 132600 loss 952831.373577\n",
      "Iter #: 132700 loss 952828.244946\n",
      "Iter #: 132800 loss 952825.120782\n",
      "Iter #: 132900 loss 952822.001076\n",
      "Iter #: 133000 loss 952818.885819\n",
      "Iter #: 133100 loss 952815.774998\n",
      "Iter #: 133200 loss 952812.668602\n",
      "Iter #: 133300 loss 952809.566617\n",
      "Iter #: 133400 loss 952806.469035\n",
      "Iter #: 133500 loss 952803.375843\n",
      "Iter #: 133600 loss 952800.287029\n",
      "Iter #: 133700 loss 952797.202582\n",
      "Iter #: 133800 loss 952794.12249\n",
      "Iter #: 133900 loss 952791.046745\n",
      "Iter #: 134000 loss 952787.975333\n",
      "Iter #: 134100 loss 952784.908244\n",
      "Iter #: 134200 loss 952781.845475\n",
      "Iter #: 134300 loss 952778.787\n",
      "Iter #: 134400 loss 952775.732814\n",
      "Iter #: 134500 loss 952772.682908\n",
      "Iter #: 134600 loss 952769.637273\n",
      "Iter #: 134700 loss 952766.595894\n",
      "Iter #: 134800 loss 952763.558763\n",
      "Iter #: 134900 loss 952760.525866\n",
      "Iter #: 135000 loss 952757.497199\n",
      "Iter #: 135100 loss 952754.472743\n",
      "Iter #: 135200 loss 952751.452491\n",
      "Iter #: 135300 loss 952748.436442\n",
      "Iter #: 135400 loss 952745.424569\n",
      "Iter #: 135500 loss 952742.416867\n",
      "Iter #: 135600 loss 952739.413328\n",
      "Iter #: 135700 loss 952736.413942\n",
      "Iter #: 135800 loss 952733.418697\n",
      "Iter #: 135900 loss 952730.427587\n",
      "Iter #: 136000 loss 952727.440597\n",
      "Iter #: 136100 loss 952724.457719\n",
      "Iter #: 136200 loss 952721.47894\n",
      "Iter #: 136300 loss 952718.504257\n",
      "Iter #: 136400 loss 952715.533651\n",
      "Iter #: 136500 loss 952712.567118\n",
      "Iter #: 136600 loss 952709.604647\n",
      "Iter #: 136700 loss 952706.646295\n",
      "Iter #: 136800 loss 952703.691914\n",
      "Iter #: 136900 loss 952700.741563\n",
      "Iter #: 137000 loss 952697.795233\n",
      "Iter #: 137100 loss 952694.852916\n",
      "Iter #: 137200 loss 952691.914601\n",
      "Iter #: 137300 loss 952688.980278\n",
      "Iter #: 137400 loss 952686.049939\n",
      "Iter #: 137500 loss 952683.123578\n",
      "Iter #: 137600 loss 952680.201175\n",
      "Iter #: 137700 loss 952677.282725\n",
      "Iter #: 137800 loss 952674.36822\n",
      "Iter #: 137900 loss 952671.457659\n",
      "Iter #: 138000 loss 952668.551015\n",
      "Iter #: 138100 loss 952665.648287\n",
      "Iter #: 138200 loss 952662.749468\n",
      "Iter #: 138300 loss 952659.854616\n",
      "Iter #: 138400 loss 952656.963581\n",
      "Iter #: 138500 loss 952654.076431\n",
      "Iter #: 138600 loss 952651.193149\n",
      "Iter #: 138700 loss 952648.313728\n",
      "Iter #: 138800 loss 952645.438153\n",
      "Iter #: 138900 loss 952642.566422\n",
      "Iter #: 139000 loss 952639.698523\n",
      "Iter #: 139100 loss 952636.834466\n",
      "Iter #: 139200 loss 952633.974207\n",
      "Iter #: 139300 loss 952631.117757\n",
      "Iter #: 139400 loss 952628.265103\n",
      "Iter #: 139500 loss 952625.416237\n",
      "Iter #: 139600 loss 952622.571151\n",
      "Iter #: 139700 loss 952619.729833\n",
      "Iter #: 139800 loss 952616.892281\n",
      "Iter #: 139900 loss 952614.058479\n",
      "Iter #: 140000 loss 952611.228426\n",
      "Iter #: 140100 loss 952608.402108\n",
      "Iter #: 140200 loss 952605.579514\n",
      "Iter #: 140300 loss 952602.760639\n",
      "Iter #: 140400 loss 952599.945475\n",
      "Iter #: 140500 loss 952597.134012\n",
      "Iter #: 140600 loss 952594.326241\n",
      "Iter #: 140700 loss 952591.522158\n",
      "Iter #: 140800 loss 952588.72175\n",
      "Iter #: 140900 loss 952585.925006\n",
      "Iter #: 141000 loss 952583.13192\n",
      "Iter #: 141100 loss 952580.34249\n",
      "Iter #: 141200 loss 952577.556698\n",
      "Iter #: 141300 loss 952574.77454\n",
      "Iter #: 141400 loss 952571.996009\n",
      "Iter #: 141500 loss 952569.221091\n",
      "Iter #: 141600 loss 952566.449785\n",
      "Iter #: 141700 loss 952563.682077\n",
      "Iter #: 141800 loss 952560.917966\n",
      "Iter #: 141900 loss 952558.157436\n",
      "Iter #: 142000 loss 952555.400484\n",
      "Iter #: 142100 loss 952552.647099\n",
      "Iter #: 142200 loss 952549.897274\n",
      "Iter #: 142300 loss 952547.151005\n",
      "Iter #: 142400 loss 952544.408281\n",
      "Iter #: 142500 loss 952541.669088\n",
      "Iter #: 142600 loss 952538.933424\n",
      "Iter #: 142700 loss 952536.201279\n",
      "Iter #: 142800 loss 952533.472649\n",
      "Iter #: 142900 loss 952530.74754\n",
      "Iter #: 143000 loss 952528.025911\n",
      "Iter #: 143100 loss 952525.307771\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter #: 143200 loss 952522.593118\n",
      "Iter #: 143300 loss 952519.88193\n",
      "Iter #: 143400 loss 952517.174214\n",
      "Iter #: 143500 loss 952514.469953\n",
      "Iter #: 143600 loss 952511.769142\n",
      "Iter #: 143700 loss 952509.071773\n",
      "Iter #: 143800 loss 952506.377838\n",
      "Iter #: 143900 loss 952503.68733\n",
      "Iter #: 144000 loss 952501.000245\n",
      "Iter #: 144100 loss 952498.316573\n",
      "Iter #: 144200 loss 952495.636304\n",
      "Iter #: 144300 loss 952492.959432\n",
      "Iter #: 144400 loss 952490.285951\n",
      "Iter #: 144500 loss 952487.615852\n",
      "Iter #: 144600 loss 952484.949126\n",
      "Iter #: 144700 loss 952482.285768\n",
      "Iter #: 144800 loss 952479.625771\n",
      "Iter #: 144900 loss 952476.969129\n",
      "Iter #: 145000 loss 952474.315834\n",
      "Iter #: 145100 loss 952471.665878\n",
      "Iter #: 145200 loss 952469.01925\n",
      "Iter #: 145300 loss 952466.375947\n",
      "Iter #: 145400 loss 952463.735961\n",
      "Iter #: 145500 loss 952461.099283\n",
      "Iter #: 145600 loss 952458.46591\n",
      "Iter #: 145700 loss 952455.835833\n",
      "Iter #: 145800 loss 952453.20904\n",
      "Iter #: 145900 loss 952450.585532\n",
      "Iter #: 146000 loss 952447.965296\n",
      "Iter #: 146100 loss 952445.348326\n",
      "Iter #: 146200 loss 952442.734618\n",
      "Iter #: 146300 loss 952440.12416\n",
      "Iter #: 146400 loss 952437.51695\n",
      "Iter #: 146500 loss 952434.912977\n",
      "Iter #: 146600 loss 952432.312237\n",
      "Iter #: 146700 loss 952429.714725\n",
      "Iter #: 146800 loss 952427.120434\n",
      "Iter #: 146900 loss 952424.529351\n",
      "Iter #: 147000 loss 952421.941471\n",
      "Iter #: 147100 loss 952419.35679\n",
      "Iter #: 147200 loss 952416.775299\n",
      "Iter #: 147300 loss 952414.196996\n",
      "Iter #: 147400 loss 952411.621868\n",
      "Iter #: 147500 loss 952409.049911\n",
      "Iter #: 147600 loss 952406.48112\n",
      "Iter #: 147700 loss 952403.915484\n",
      "Iter #: 147800 loss 952401.353001\n",
      "Iter #: 147900 loss 952398.79366\n",
      "Iter #: 148000 loss 952396.237456\n",
      "Iter #: 148100 loss 952393.684384\n",
      "Iter #: 148200 loss 952391.134437\n",
      "Iter #: 148300 loss 952388.587609\n",
      "Iter #: 148400 loss 952386.043891\n",
      "Iter #: 148500 loss 952383.503278\n",
      "Iter #: 148600 loss 952380.965764\n",
      "Iter #: 148700 loss 952378.431349\n",
      "Iter #: 148800 loss 952375.900011\n",
      "Iter #: 148900 loss 952373.37176\n",
      "Iter #: 149000 loss 952370.846575\n",
      "Iter #: 149100 loss 952368.324455\n",
      "Iter #: 149200 loss 952365.805403\n",
      "Iter #: 149300 loss 952363.289399\n",
      "Iter #: 149400 loss 952360.776445\n",
      "Iter #: 149500 loss 952358.266527\n",
      "Iter #: 149600 loss 952355.759646\n",
      "Iter #: 149700 loss 952353.255793\n",
      "Iter #: 149800 loss 952350.754962\n",
      "Iter #: 149900 loss 952348.257154\n",
      "Iter #: 150000 loss 952345.762353\n",
      "Iter #: 150100 loss 952343.270553\n",
      "Iter #: 150200 loss 952340.781748\n",
      "Iter #: 150300 loss 952338.295936\n",
      "Iter #: 150400 loss 952335.81311\n",
      "Iter #: 150500 loss 952333.333261\n",
      "Iter #: 150600 loss 952330.856387\n",
      "Iter #: 150700 loss 952328.382479\n",
      "Iter #: 150800 loss 952325.911535\n",
      "Iter #: 150900 loss 952323.443545\n",
      "Iter #: 151000 loss 952320.978502\n",
      "Iter #: 151100 loss 952318.516406\n",
      "Iter #: 151200 loss 952316.057244\n",
      "Iter #: 151300 loss 952313.601013\n",
      "Iter #: 151400 loss 952311.147706\n",
      "Iter #: 151500 loss 952308.697318\n",
      "Iter #: 151600 loss 952306.249844\n",
      "Iter #: 151700 loss 952303.805278\n",
      "Iter #: 151800 loss 952301.363618\n",
      "Iter #: 151900 loss 952298.924853\n",
      "Iter #: 152000 loss 952296.488981\n",
      "Iter #: 152100 loss 952294.055986\n",
      "Iter #: 152200 loss 952291.625869\n",
      "Iter #: 152300 loss 952289.198628\n",
      "Iter #: 152400 loss 952286.774254\n",
      "Iter #: 152500 loss 952284.352744\n",
      "Iter #: 152600 loss 952281.934086\n",
      "Iter #: 152700 loss 952279.518277\n",
      "Iter #: 152800 loss 952277.105311\n",
      "Iter #: 152900 loss 952274.695185\n",
      "Iter #: 153000 loss 952272.287895\n",
      "Iter #: 153100 loss 952269.883428\n",
      "Iter #: 153200 loss 952267.481783\n",
      "Iter #: 153300 loss 952265.082952\n",
      "Iter #: 153400 loss 952262.686934\n",
      "Iter #: 153500 loss 952260.293722\n",
      "Iter #: 153600 loss 952257.903308\n",
      "Iter #: 153700 loss 952255.515688\n",
      "Iter #: 153800 loss 952253.13086\n",
      "Iter #: 153900 loss 952250.74881\n",
      "Iter #: 154000 loss 952248.369539\n",
      "Iter #: 154100 loss 952245.993063\n",
      "Iter #: 154200 loss 952243.619328\n",
      "Iter #: 154300 loss 952241.248356\n",
      "Iter #: 154400 loss 952238.880138\n",
      "Iter #: 154500 loss 952236.514672\n",
      "Iter #: 154600 loss 952234.151952\n",
      "Iter #: 154700 loss 952231.79197\n",
      "Iter #: 154800 loss 952229.434721\n",
      "Iter #: 154900 loss 952227.080205\n",
      "Iter #: 155000 loss 952224.728408\n",
      "Iter #: 155100 loss 952222.379336\n",
      "Iter #: 155200 loss 952220.03297\n",
      "Iter #: 155300 loss 952217.689312\n",
      "Iter #: 155400 loss 952215.348357\n",
      "Iter #: 155500 loss 952213.010099\n",
      "Iter #: 155600 loss 952210.674543\n",
      "Iter #: 155700 loss 952208.341664\n",
      "Iter #: 155800 loss 952206.011474\n",
      "Iter #: 155900 loss 952203.683956\n",
      "Iter #: 156000 loss 952201.359106\n",
      "Iter #: 156100 loss 952199.036923\n",
      "Iter #: 156200 loss 952196.7174\n",
      "Iter #: 156300 loss 952194.400533\n",
      "Iter #: 156400 loss 952192.086321\n",
      "Iter #: 156500 loss 952189.774752\n",
      "Iter #: 156600 loss 952187.465823\n",
      "Iter #: 156700 loss 952185.159531\n",
      "Iter #: 156800 loss 952182.855869\n",
      "Iter #: 156900 loss 952180.554836\n",
      "Iter #: 157000 loss 952178.256423\n",
      "Iter #: 157100 loss 952175.960622\n",
      "Iter #: 157200 loss 952173.667434\n",
      "Iter #: 157300 loss 952171.376856\n",
      "Iter #: 157400 loss 952169.088875\n",
      "Iter #: 157500 loss 952166.803486\n",
      "Iter #: 157600 loss 952164.520694\n",
      "Iter #: 157700 loss 952162.240483\n",
      "Iter #: 157800 loss 952159.962853\n",
      "Iter #: 157900 loss 952157.687799\n",
      "Iter #: 158000 loss 952155.415325\n",
      "Iter #: 158100 loss 952153.14541\n",
      "Iter #: 158200 loss 952150.878059\n",
      "Iter #: 158300 loss 952148.613261\n",
      "Iter #: 158400 loss 952146.351015\n",
      "Iter #: 158500 loss 952144.091322\n",
      "Iter #: 158600 loss 952141.834167\n",
      "Iter #: 158700 loss 952139.579555\n",
      "Iter #: 158800 loss 952137.327473\n",
      "Iter #: 158900 loss 952135.077919\n",
      "Iter #: 159000 loss 952132.830887\n",
      "Iter #: 159100 loss 952130.586375\n",
      "Iter #: 159200 loss 952128.344376\n",
      "Iter #: 159300 loss 952126.104892\n",
      "Iter #: 159400 loss 952123.867908\n",
      "Iter #: 159500 loss 952121.633423\n",
      "Iter #: 159600 loss 952119.401433\n",
      "Iter #: 159700 loss 952117.171945\n",
      "Iter #: 159800 loss 952114.944935\n",
      "Iter #: 159900 loss 952112.720406\n",
      "Iter #: 160000 loss 952110.498355\n",
      "Iter #: 160100 loss 952108.278774\n",
      "Iter #: 160200 loss 952106.061661\n",
      "Iter #: 160300 loss 952103.847015\n",
      "Iter #: 160400 loss 952101.634823\n",
      "Iter #: 160500 loss 952099.425086\n",
      "Iter #: 160600 loss 952097.217808\n",
      "Iter #: 160700 loss 952095.012975\n",
      "Iter #: 160800 loss 952092.81059\n",
      "Iter #: 160900 loss 952090.610628\n",
      "Iter #: 161000 loss 952088.413096\n",
      "Iter #: 161100 loss 952086.217993\n",
      "Iter #: 161200 loss 952084.025318\n",
      "Iter #: 161300 loss 952081.835061\n",
      "Iter #: 161400 loss 952079.647216\n",
      "Iter #: 161500 loss 952077.461779\n",
      "Iter #: 161600 loss 952075.278745\n",
      "Iter #: 161700 loss 952073.098114\n",
      "Iter #: 161800 loss 952070.919881\n",
      "Iter #: 161900 loss 952068.744038\n",
      "Iter #: 162000 loss 952066.570587\n",
      "Iter #: 162100 loss 952064.399516\n",
      "Iter #: 162200 loss 952062.230822\n",
      "Iter #: 162300 loss 952060.064505\n",
      "Iter #: 162400 loss 952057.900563\n",
      "Iter #: 162500 loss 952055.738982\n",
      "Iter #: 162600 loss 952053.579767\n",
      "Iter #: 162700 loss 952051.422909\n",
      "Iter #: 162800 loss 952049.268406\n",
      "Iter #: 162900 loss 952047.116253\n",
      "Iter #: 163000 loss 952044.966443\n",
      "Iter #: 163100 loss 952042.818978\n",
      "Iter #: 163200 loss 952040.673846\n",
      "Iter #: 163300 loss 952038.53105\n",
      "Iter #: 163400 loss 952036.390582\n",
      "Iter #: 163500 loss 952034.252437\n",
      "Iter #: 163600 loss 952032.116623\n",
      "Iter #: 163700 loss 952029.983112\n",
      "Iter #: 163800 loss 952027.851916\n",
      "Iter #: 163900 loss 952025.723029\n",
      "Iter #: 164000 loss 952023.596442\n",
      "Iter #: 164100 loss 952021.472154\n",
      "Iter #: 164200 loss 952019.350165\n",
      "Iter #: 164300 loss 952017.230464\n",
      "Iter #: 164400 loss 952015.113053\n",
      "Iter #: 164500 loss 952012.997931\n",
      "Iter #: 164600 loss 952010.885095\n",
      "Iter #: 164700 loss 952008.77452\n",
      "Iter #: 164800 loss 952006.666215\n",
      "Iter #: 164900 loss 952004.560185\n",
      "Iter #: 165000 loss 952002.456416\n",
      "Iter #: 165100 loss 952000.354904\n",
      "Iter #: 165200 loss 951998.255645\n",
      "Iter #: 165300 loss 951996.158637\n",
      "Iter #: 165400 loss 951994.063877\n",
      "Iter #: 165500 loss 951991.971364\n",
      "Iter #: 165600 loss 951989.881093\n",
      "Iter #: 165700 loss 951987.793062\n",
      "Iter #: 165800 loss 951985.707255\n",
      "Iter #: 165900 loss 951983.62368\n",
      "Iter #: 166000 loss 951981.542327\n",
      "Iter #: 166100 loss 951979.463191\n",
      "Iter #: 166200 loss 951977.386271\n",
      "Iter #: 166300 loss 951975.311565\n",
      "Iter #: 166400 loss 951973.239068\n",
      "Iter #: 166500 loss 951971.168777\n",
      "Iter #: 166600 loss 951969.100692\n",
      "Iter #: 166700 loss 951967.0348\n",
      "Iter #: 166800 loss 951964.971105\n",
      "Iter #: 166900 loss 951962.909593\n",
      "Iter #: 167000 loss 951960.850268\n",
      "Iter #: 167100 loss 951958.793128\n",
      "Iter #: 167200 loss 951956.738173\n",
      "Iter #: 167300 loss 951954.685388\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter #: 167400 loss 951952.634773\n",
      "Iter #: 167500 loss 951950.586318\n",
      "Iter #: 167600 loss 951948.540024\n",
      "Iter #: 167700 loss 951946.495893\n",
      "Iter #: 167800 loss 951944.453918\n",
      "Iter #: 167900 loss 951942.414093\n",
      "Iter #: 168000 loss 951940.376417\n",
      "Iter #: 168100 loss 951938.340888\n",
      "Iter #: 168200 loss 951936.307497\n",
      "Iter #: 168300 loss 951934.276248\n",
      "Iter #: 168400 loss 951932.247133\n",
      "Iter #: 168500 loss 951930.220144\n",
      "Iter #: 168600 loss 951928.195281\n",
      "Iter #: 168700 loss 951926.172543\n",
      "Iter #: 168800 loss 951924.151921\n",
      "Iter #: 168900 loss 951922.133421\n",
      "Iter #: 169000 loss 951920.117028\n",
      "--- 1649.7420620918274 seconds ---\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "start_time = time.time()\n",
    "opt_experiment(model_enwik,\n",
    "               mode='AM', \n",
    "               d=dimension,\n",
    "               eta = 1e-5,\n",
    "               MAX_ITER=50000,\n",
    "               from_iter=119000,\n",
    "               start_from='SVD',\n",
    "               init=(True, C_svd, W_svd), display=True)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_enwik.C = C_svd\n",
    "model_enwik.W = W_svd\n",
    "start_time = time.time()\n",
    "model_enwik.bfgd(d=dimension,from_iter=10000, MAX_ITER=10000, eta=5e-6, display=True,\n",
    "                 init=(True, C_svd, W_svd), \n",
    "                 save=[True, 'dataset'])\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''model = Word2Vec(real_sentences, size=200, window=5, min_count=5, workers=4)\n",
    "fname = 'original'\n",
    "model.save(fname)\n",
    "model = Word2Vec.load(fname)  # you can continue training with the loaded model!'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
