{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "from scipy.sparse.linalg import svds\n",
    "from numpy.linalg import norm, svd\n",
    "\n",
    "import itertools\n",
    "import math\n",
    "import re\n",
    "\n",
    "#from bs4 import BeautifulSoup\n",
    "#from nltk.corpus import stopwords\n",
    "#from gensim.models import word2vec, Word2Vec\n",
    "\n",
    "from util.word2vec_as_MF import Word2vecMF\n",
    "from util.functions import *\n",
    "\n",
    "import time\n",
    "dimension = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read and preprocess enwik9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing sentences from training set\n"
     ]
    }
   ],
   "source": [
    "real_sentences = load('data/x1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'%%time\\nskip = Word2Vec(real_sentences, size = dimension, compute_loss=True)'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''%%time\n",
    "skip = Word2Vec(real_sentences, size = dimension, compute_loss=True)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'skip.get_latest_training_loss()'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''skip.get_latest_training_loss()'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train ro_sgns model starting from SVD of SPPMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create word2vec as matrix factorization model\n",
    "model_enwik = Word2vecMF()\n",
    "#model_enwik.data_to_matrices(real_sentences, dimension, 5, 'enwik-200/matrices.npz')\n",
    "model_enwik.data_to_matrices(real_sentences, dimension, 5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"model_enwik = Word2vecMF()\\nmodel_enwik.load_matrices(from_file='enwik-200/matrices1.npz')\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If the model has been already created, load it from file\n",
    "'''model_enwik = Word2vecMF()\n",
    "model_enwik.load_matrices(from_file='enwik-200/matrices1.npz')'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# SVD initialization\\nSPPMI = np.maximum(np.log(model_enwik.D) - np.log(model_enwik.B), 0)\\n# print SPPMI\\nu, s, vt = svds(SPPMI, k=dimension)\\nC_svd = u.dot(np.sqrt(np.diag(s))).T\\nW_svd = np.sqrt(np.diag(s)).dot(vt)\\nprint(model_enwik.MF(C_svd, W_svd))'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# SVD initialization\n",
    "SPPMI = np.maximum(np.log(model_enwik.D) - np.log(model_enwik.B), 0)\n",
    "# print SPPMI\n",
    "u, s, vt = svds(SPPMI, k=dimension)\n",
    "C_svd = u.dot(np.sqrt(np.diag(s))).T\n",
    "W_svd = np.sqrt(np.diag(s)).dot(vt)\n",
    "print(model_enwik.MF(C_svd, W_svd))'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"model_enwik.C = C_svd\\nmodel_enwik.W = W_svd\\n#model_enwik.save_CW('enwik-200/initializations/SVD_dim200', 0)\\nprint(model_enwik.C.shape, model_enwik.W.shape, model_enwik.B.shape, model_enwik.D.shape)\""
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''model_enwik.C = C_svd\n",
    "model_enwik.W = W_svd\n",
    "#model_enwik.save_CW('enwik-200/initializations/SVD_dim200', 0)\n",
    "print(model_enwik.C.shape, model_enwik.W.shape, model_enwik.B.shape, model_enwik.D.shape)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(356, 356)\n",
      "(1783763.7762035725, 5.251327505899099e-14)\n"
     ]
    }
   ],
   "source": [
    "X0 = 1/norm((model_enwik.B+model_enwik.D)/4, 'fro')*model_enwik.grad_MF(\n",
    "    np.zeros([dimension,model_enwik.B.shape[0]]), np.zeros([dimension,model_enwik.B.shape[0]]))\n",
    "print(X0.shape)\n",
    "u, s, vt = svds(X0, k=dimension)\n",
    "C0 = u.dot(np.sqrt(np.diag(s))).T\n",
    "W0 = np.sqrt(np.diag(s)).dot(vt)\n",
    "print(model_enwik.MF(C0, W0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Train the model\\nstart_time = time.time()\\nopt_experiment(model_enwik,\\n               mode=\\'PS\\', \\n               d=dimension,\\n               eta = 5e-5,\\n               MAX_ITER=10,\\n               from_iter=0,\\n               start_from=\\'SVD\\',\\n               init=(True, C_svd, W_svd), display=True)\\nprint(\"--- %s seconds ---\" % (time.time() - start_time))'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# Train the model\n",
    "start_time = time.time()\n",
    "opt_experiment(model_enwik,\n",
    "               mode='PS', \n",
    "               d=dimension,\n",
    "               eta = 5e-5,\n",
    "               MAX_ITER=10,\n",
    "               from_iter=0,\n",
    "               start_from='SVD',\n",
    "               init=(True, C_svd, W_svd), display=True)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Train the model\\nstart_time = time.time()\\nopt_experiment(model_enwik,\\n               mode=\\'AM\\', \\n               d=dimension,\\n               eta = 5e-6,\\n               lbd = 1.0,\\n               MAX_ITER=189000,\\n               from_iter=0,\\n               start_from=\\'SVD\\',\\n               init=(True, C_svd, W_svd), display=True)\\nprint(\"--- %s seconds ---\" % (time.time() - start_time))'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# Train the model\n",
    "start_time = time.time()\n",
    "opt_experiment(model_enwik,\n",
    "               mode='AM', \n",
    "               d=dimension,\n",
    "               eta = 5e-6,\n",
    "               lbd = 1.0,\n",
    "               MAX_ITER=189000,\n",
    "               from_iter=0,\n",
    "               start_from='SVD',\n",
    "               init=(True, C_svd, W_svd), display=True)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enwik-200/BFGDiter_fromX0_full_dim100_step1e-06_0.006\n",
      "Iter #: 100 loss (1621255.1016031264, 0.002798092004622574)\n",
      "Iter #: 200 loss (1497570.5076319438, 0.0021177411148418234)\n",
      "Iter #: 300 loss (1413499.8261292032, 0.0012984027934174372)\n",
      "Iter #: 400 loss (1353973.041003951, 0.0014624949619139564)\n",
      "Iter #: 500 loss (1308949.5045622494, 0.0033527840878211537)\n",
      "Iter #: 600 loss (1273067.5570341928, 0.0058166326070082566)\n",
      "Iter #: 700 loss (1243558.9755853657, 0.008469237218521582)\n",
      "Iter #: 800 loss (1218823.103892192, 0.011084429714644125)\n",
      "Iter #: 900 loss (1197833.2922820796, 0.013538762351705423)\n",
      "Iter #: 1000 loss (1179911.979329155, 0.0157521841235404)\n",
      "Iter #: 1100 loss (1164605.7055433504, 0.017677197516062934)\n",
      "Iter #: 1200 loss (1151576.839455201, 0.019305652194758664)\n",
      "Iter #: 1300 loss (1140526.6630331478, 0.020661943580257466)\n",
      "Iter #: 1400 loss (1131166.5686767232, 0.021784891360438766)\n",
      "Iter #: 1500 loss (1123223.559605428, 0.02271290841442468)\n",
      "Iter #: 1600 loss (1116453.6342020906, 0.023478359957050752)\n",
      "Iter #: 1700 loss (1110648.732609905, 0.024107670986221488)\n",
      "Iter #: 1800 loss (1105636.4124362918, 0.024622815423686273)\n",
      "Iter #: 1900 loss (1101275.7551662605, 0.025042414271067796)\n",
      "Iter #: 2000 loss (1097452.2458509165, 0.025382325684280463)\n",
      "Iter #: 2100 loss (1094072.9210845558, 0.02565599110327338)\n",
      "Iter #: 2200 loss (1091062.2107338654, 0.025874714823509424)\n",
      "Iter #: 2300 loss (1088358.524248277, 0.026047936623358234)\n",
      "Iter #: 2400 loss (1085911.5043865356, 0.026183499822957767)\n",
      "Iter #: 2500 loss (1083679.8414377132, 0.02628790241232485)\n",
      "Iter #: 2600 loss (1081629.5445149082, 0.026366521183635843)\n",
      "Iter #: 2700 loss (1079732.5791097886, 0.02642380463685158)\n",
      "Iter #: 2800 loss (1077965.7939384948, 0.02646343505756529)\n",
      "Iter #: 2900 loss (1076310.0729743256, 0.02648846276231119)\n",
      "Iter #: 3000 loss (1074749.660015394, 0.02650141652708471)\n",
      "Iter #: 3100 loss (1073271.6132524293, 0.02650439427836343)\n",
      "Iter #: 3200 loss (1071865.356282543, 0.026499137714884796)\n",
      "Iter #: 3300 loss (1070522.3000384327, 0.026487093943590383)\n",
      "Iter #: 3400 loss (1069235.5172510322, 0.026469466607281562)\n",
      "Iter #: 3500 loss (1067999.457284389, 0.026447258435702403)\n",
      "Iter #: 3600 loss (1066809.694302433, 0.026421306700332364)\n",
      "Iter #: 3700 loss (1065662.7055297913, 0.02639231269583903)\n",
      "Iter #: 3800 loss (1064555.6786993933, 0.026360866120538944)\n",
      "Iter #: 3900 loss (1063486.3486576695, 0.026327465058250305)\n",
      "Iter #: 4000 loss (1062452.8627702196, 0.02629253216798806)\n",
      "Iter #: 4100 loss (1061453.6736802135, 0.02625642764161862)\n",
      "Iter #: 4200 loss (1060487.4566502355, 0.02621945946137548)\n",
      "Iter #: 4300 loss (1059553.0476430051, 0.02618189147037282)\n",
      "Iter #: 4400 loss (1058649.3977722637, 0.02614394972891591)\n",
      "Iter #: 4500 loss (1057775.5398640898, 0.026105827579570116)\n",
      "Iter #: 4600 loss (1056930.5634979035, 0.026067689768533242)\n",
      "Iter #: 4700 loss (1056113.5958169575, 0.026029675896372145)\n",
      "Iter #: 4800 loss (1055323.7863614738, 0.02599190338973798)\n",
      "Iter #: 4900 loss (1054560.2949883936, 0.025954470123591054)\n",
      "Iter #: 5000 loss (1053822.2824952784, 0.025917456770817835)\n",
      "Iter #: 5100 loss (1053108.903847833, 0.025880928921902805)\n",
      "Iter #: 5200 loss (1052419.3039720976, 0.02584493899697688)\n",
      "Iter #: 5300 loss (1051752.6159963354, 0.02580952796799693)\n",
      "Iter #: 5400 loss (1051107.9616972608, 0.025774726898385645)\n",
      "Iter #: 5500 loss (1050484.4537842125, 0.025740558317888025)\n",
      "Iter #: 5600 loss (1049881.1995792892, 0.025707037443834558)\n",
      "Iter #: 5700 loss (1049297.3056319077, 0.025674173264751774)\n",
      "Iter #: 5800 loss (1048731.8828354701, 0.025641969501438447)\n",
      "Iter #: 5900 loss (1048184.0516757605, 0.02561042545847762)\n",
      "Iter #: 6000 loss (1047652.9473179516, 0.02557953677746665)\n",
      "Iter #: 6100 loss (1047137.7243175812, 0.025549296103387093)\n",
      "Iter #: 6200 loss (1046637.560811581, 0.02551969367003388)\n",
      "Iter #: 6300 loss (1046151.6621041566, 0.025490717813173802)\n",
      "Iter #: 6400 loss (1045679.2636083689, 0.025462355416834222)\n",
      "Iter #: 6500 loss (1045219.6331389744, 0.0254345922976242)\n",
      "Iter #: 6600 loss (1044772.0725774562, 0.025407413533178363)\n",
      "Iter #: 6700 loss (1044335.9189483332, 0.025380803738151665)\n",
      "Iter #: 6800 loss (1043910.5449583003, 0.02535474729488801)\n",
      "Iter #: 6900 loss (1043495.3590577573, 0.025329228540884867)\n",
      "Iter #: 7000 loss (1043089.8050885906, 0.025304231919380096)\n",
      "Iter #: 7100 loss (1042693.3615832624, 0.02527974209754681)\n",
      "Iter #: 7200 loss (1042305.5407788741, 0.025255744055880817)\n",
      "Iter #: 7300 loss (1041925.8874063485, 0.025232223153259844)\n",
      "Iter #: 7400 loss (1041553.9773098006, 0.02520916517191967)\n",
      "Iter #: 7500 loss (1041189.4159450044, 0.02518655634523775)\n",
      "Iter #: 7600 loss (1040831.8367992032, 0.0251643833719)\n",
      "Iter #: 7700 loss (1040480.8997677057, 0.02514263341941342)\n",
      "Iter #: 7800 loss (1040136.2895162593, 0.025121294118741047)\n",
      "Iter #: 7900 loss (1039797.7138522351, 0.025100353553536115)\n",
      "Iter #: 8000 loss (1039464.9021225017, 0.025079800244064702)\n",
      "Iter #: 8100 loss (1039137.6036514894, 0.02505962312927836)\n",
      "Iter #: 8200 loss (1038815.5862294126, 0.025039811546082744)\n",
      "Iter #: 8300 loss (1038498.6346578625, 0.025020355209137896)\n",
      "Iter #: 8400 loss (1038186.5493579004, 0.025001244189499762)\n",
      "Iter #: 8500 loss (1037879.1450442601, 0.02498246889428039)\n",
      "Iter #: 8600 loss (1037576.2494681993, 0.02496402004745151)\n",
      "Iter #: 8700 loss (1037277.7022307692, 0.02494588867192678)\n",
      "Iter #: 8800 loss (1036983.3536677458, 0.02492806607205614)\n",
      "Iter #: 8900 loss (1036693.0638070285, 0.024910543818647535)\n",
      "Iter #: 9000 loss (1036406.7013989504, 0.02489331373451564)\n",
      "Iter #: 9100 loss (1036124.1430196074, 0.024876367882274156)\n",
      "Iter #: 9200 loss (1035845.2722468904, 0.024859698551942224)\n",
      "Iter #: 9300 loss (1035569.9789085297, 0.024843298251713886)\n",
      "Iter #: 9400 loss (1035298.1584009829, 0.02482715969785148)\n",
      "Iter #: 9500 loss (1035029.7110775183, 0.024811275806968684)\n",
      "Iter #: 9600 loss (1034764.5417033728, 0.024795639688461702)\n",
      "Iter #: 9700 loss (1034502.5589753638, 0.024780244638087583)\n",
      "Iter #: 9800 loss (1034243.6751028982, 0.024765084131564176)\n",
      "Iter #: 9900 loss (1033987.8054469156, 0.024750151819890782)\n",
      "Iter #: 10000 loss (1033734.8682129552, 0.02473544152424397)\n",
      "Iter #: 10100 loss (1033484.7841942811, 0.024720947231136433)\n",
      "Iter #: 10200 loss (1033237.4765607995, 0.024706663089147943)\n",
      "Iter #: 10300 loss (1032992.8706894026, 0.024692583404278098)\n",
      "Iter #: 10400 loss (1032750.8940313304, 0.024678702637324048)\n",
      "Iter #: 10500 loss (1032511.4760122044, 0.02466501539961087)\n",
      "Iter #: 10600 loss (1032274.5479604646, 0.024651516450519308)\n",
      "Iter #: 10700 loss (1032040.0430601274, 0.024638200693821555)\n",
      "Iter #: 10800 loss (1031807.8963239652, 0.02462506317513369)\n",
      "Iter #: 10900 loss (1031578.044583462, 0.02461209907856988)\n",
      "Iter #: 11000 loss (1031350.426492138, 0.024599303724113668)\n",
      "Iter #: 11100 loss (1031124.9825391353, 0.024586672564146077)\n",
      "Iter #: 11200 loss (1030901.6550702128, 0.024574201181310405)\n",
      "Iter #: 11300 loss (1030680.3883135947, 0.024561885285040774)\n",
      "Iter #: 11400 loss (1030461.1284083822, 0.024549720708470862)\n",
      "Iter #: 11500 loss (1030243.8234335071, 0.024537703406180354)\n",
      "Iter #: 11600 loss (1030028.4234354594, 0.024525829450481536)\n",
      "Iter #: 11700 loss (1029814.8804532571, 0.024514095028989605)\n",
      "Iter #: 11800 loss (1029603.1485393576, 0.024502496441526456)\n",
      "Iter #: 11900 loss (1029393.1837754126, 0.02449103009693135)\n",
      "Iter #: 12000 loss (1029184.9442819778, 0.024479692510850547)\n",
      "Iter #: 12100 loss (1028978.3902214518, 0.024468480301848852)\n",
      "Iter #: 12200 loss (1028773.4837937099, 0.02445739018951861)\n",
      "Iter #: 12300 loss (1028570.1892240345, 0.024446418990839217)\n",
      "Iter #: 12400 loss (1028368.4727431044, 0.024435563617957467)\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "opt_experiment(model_enwik,\n",
    "               mode='BFGD',\n",
    "               d=dimension,\n",
    "               eta = 1e-6,\n",
    "               reg = 0.006,\n",
    "               MAX_ITER=190000,\n",
    "               from_iter=0,\n",
    "               start_from='X0_full',\n",
    "               init=(True, C0, W0), display=True)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''model = Word2Vec(real_sentences, size=200, window=5, min_count=5, workers=4)\n",
    "fname = 'original'\n",
    "model.save(fname)\n",
    "model = Word2Vec.load(fname)  # you can continue training with the loaded model!'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
