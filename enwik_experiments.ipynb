{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse.linalg import svds\n",
    "\n",
    "import itertools\n",
    "import pickle\n",
    "import math\n",
    "import re\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import word2vec\n",
    "\n",
    "from word2vec_as_MF import Word2vecMF\n",
    "from functions import *\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read and preprocess enwik9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load enwik 9\n",
    "# data = np.loadtxt(\"data/enwik9.txt\", dtype=str, delimiter='.')\n",
    "data = np.loadtxt(\"data/xaa\", dtype=str, delimiter='.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wiki_to_wordlist(sentence, remove_stopwords=False ):\n",
    "    # Function to convert a document to a sequence of words,\n",
    "    # optionally removing stop words.  Returns a list of words.\n",
    "\n",
    "    # 3. Convert words to lower case and split them\n",
    "    words = sentence.split()\n",
    "    #\n",
    "    # 4. Optionally remove stop words (false by default)\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w in stops]\n",
    "    #\n",
    "    # 5. Return a list of words\n",
    "    return(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing sentences from training set\n"
     ]
    }
   ],
   "source": [
    "sentences = []  # Initialize an empty list of sentences\n",
    "\n",
    "print(\"Parsing sentences from training set\")\n",
    "for sentence in data:\n",
    "    sentences += [wiki_to_wordlist(sentence)]\n",
    "\n",
    "indices = []\n",
    "for i, sentence in enumerate(sentences):\n",
    "    if not sentence:\n",
    "        pass\n",
    "    else:\n",
    "        indices.append(i)\n",
    "\n",
    "real_sentences = np.array(sentences)[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create word2vec as matrix factorization model\n",
    "model_enwik = Word2vecMF()\n",
    "model_enwik.data_to_matrices(real_sentences, 200, 5, 'enwik-200/matrices.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the model has been already created, load it from file\n",
    "model_enwik = Word2vecMF()\n",
    "model_enwik.load_matrices(from_file='enwik-200/matrices.npz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train ro_sgns model starting from SVD of SPPMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yerong/local/Anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:2: RuntimeWarning: divide by zero encountered in log\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# SVD initialization\n",
    "SPPMI = np.maximum(np.log(model_enwik.D) - np.log(model_enwik.B), 0)\n",
    "# print SPPMI\n",
    "u, s, vt = svds(SPPMI, k=200)\n",
    "C_svd = u.dot(np.sqrt(np.diag(s))).T\n",
    "W_svd = np.sqrt(np.diag(s)).dot(vt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_enwik.C = C_svd\n",
    "model_enwik.W = W_svd\n",
    "\n",
    "model_enwik.save_CW('enwik-200/initializations/SVD_dim200', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter #: 1 loss 14.5709831941\n",
      "Iter #: 2 loss 14.5709831941\n",
      "Iter #: 3 loss 14.5461437313\n",
      "Iter #: 4 loss 14.5072837444\n",
      "Iter #: 5 loss 14.395312055\n",
      "Iter #: 6 loss 14.1393172118\n",
      "Iter #: 7 loss 13.8474359115\n",
      "Iter #: 8 loss 13.6249103605\n",
      "Iter #: 9 loss 13.4567772505\n",
      "Iter #: 10 loss 13.322318689\n",
      "Iter #: 11 loss 13.2091472975\n",
      "Iter #: 12 loss 13.1097102004\n",
      "Iter #: 13 loss 13.0189718647\n",
      "Iter #: 14 loss 12.9333205818\n",
      "Iter #: 15 loss 12.8502085526\n",
      "Iter #: 16 loss 12.7682466503\n",
      "Iter #: 17 loss 12.687422071\n",
      "Iter #: 18 loss 12.6089647989\n",
      "Iter #: 19 loss 12.5346022735\n",
      "Iter #: 20 loss 12.4655893821\n",
      "Iter #: 21 loss 12.402235817\n",
      "Iter #: 22 loss 12.3441287405\n",
      "Iter #: 23 loss 12.2905916542\n",
      "Iter #: 24 loss 12.2409690029\n",
      "Iter #: 25 loss 12.1947096576\n",
      "Iter #: 26 loss 12.1513643419\n",
      "Iter #: 27 loss 12.1105655208\n",
      "Iter #: 28 loss 12.0720092096\n",
      "Iter #: 29 loss 12.0354412093\n",
      "Iter #: 30 loss 12.000646967\n",
      "Iter #: 31 loss 11.9674440342\n",
      "Iter #: 32 loss 11.9356763366\n",
      "Iter #: 33 loss 11.9052097134\n",
      "Iter #: 34 loss 11.8759283706\n",
      "Iter #: 35 loss 11.8477320091\n",
      "Iter #: 36 loss 11.8205334685\n",
      "Iter #: 37 loss 11.7942567664\n",
      "Iter #: 38 loss 11.7688354448\n",
      "Iter #: 39 loss 11.7442111575\n",
      "Iter #: 40 loss 11.7203324487\n",
      "Iter #: 41 loss 11.6971536895\n",
      "Iter #: 42 loss 11.6746341538\n",
      "Iter #: 43 loss 11.6527372202\n",
      "Iter #: 44 loss 11.6314296963\n",
      "Iter #: 45 loss 11.6106812615\n",
      "Iter #: 46 loss 11.5904640217\n",
      "Iter #: 47 loss 11.5707521682\n",
      "Iter #: 48 loss 11.5515217264\n",
      "Iter #: 49 loss 11.5327503771\n",
      "Iter #: 50 loss 11.5144173314\n",
      "Iter #: 51 loss 11.4965032404\n",
      "Iter #: 52 loss 11.4789901196\n",
      "Iter #: 53 loss 11.461861277\n",
      "Iter #: 54 loss 11.4451012318\n",
      "Iter #: 55 loss 11.4286956209\n",
      "Iter #: 56 loss 11.4126310911\n",
      "Iter #: 57 loss 11.3968951805\n",
      "Iter #: 58 loss 11.3814761958\n",
      "Iter #: 59 loss 11.3663630905\n",
      "Iter #: 60 loss 11.351545353\n",
      "Iter #: 61 loss 11.3370129096\n",
      "Iter #: 62 loss 11.3227560462\n",
      "Iter #: 63 loss 11.3087653507\n",
      "Iter #: 64 loss 11.2950316757\n",
      "Iter #: 65 loss 11.281546119\n",
      "Iter #: 66 loss 11.268300018\n",
      "Iter #: 67 loss 11.2552849551\n",
      "Iter #: 68 loss 11.2424927688\n",
      "Iter #: 69 loss 11.2299155674\n",
      "Iter #: 70 loss 11.217545743\n",
      "Iter #: 71 loss 11.2053759824\n",
      "Iter #: 72 loss 11.1933992754\n",
      "Iter #: 73 loss 11.1816089186\n",
      "Iter #: 74 loss 11.1699985147\n",
      "Iter #: 75 loss 11.1585619693\n",
      "Iter #: 76 loss 11.1472934831\n",
      "Iter #: 77 loss 11.1361875426\n",
      "Iter #: 78 loss 11.1252389082\n",
      "Iter #: 79 loss 11.1144426021\n",
      "Iter #: 80 loss 11.1037938938\n",
      "Iter #: 81 loss 11.0932882871\n",
      "Iter #: 82 loss 11.0829215062\n",
      "Iter #: 83 loss 11.072689482\n",
      "Iter #: 84 loss 11.0625883396\n",
      "Iter #: 85 loss 11.0526143858\n",
      "Iter #: 86 loss 11.0427640976\n",
      "Iter #: 87 loss 11.0330341117\n",
      "Iter #: 88 loss 11.023421214\n",
      "Iter #: 89 loss 11.0139223306\n",
      "Iter #: 90 loss 11.0045345191\n",
      "Iter #: 91 loss 10.9952549605\n",
      "Iter #: 92 loss 10.9860809522\n",
      "Iter #: 93 loss 10.9770099007\n",
      "Iter #: 94 loss 10.9680393158\n",
      "Iter #: 95 loss 10.9591668048\n",
      "Iter #: 96 loss 10.9503900668\n",
      "Iter #: 97 loss 10.941706888\n",
      "Iter #: 98 loss 10.9331151374\n",
      "Iter #: 99 loss 10.924612762\n",
      "Iter #: 100 loss 10.9161977833\n",
      "--- 147.9742534160614 seconds ---\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "start_time = time.time()\n",
    "opt_experiment(model_enwik,\n",
    "               mode='PS', \n",
    "               d=200,\n",
    "               eta = 5e-5,\n",
    "               MAX_ITER=100,\n",
    "               from_iter=0,\n",
    "               start_from='SVD',\n",
    "               init=(True, C_svd, W_svd), display=True)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Itecr #: 1 loss 9.16735031547\n",
      "Itecr #: 2 loss 9.70276310932\n",
      "Itecr #: 3 loss 9.37616560721\n",
      "Itecr #: 4 loss 9.86247403949\n",
      "Itecr #: 5 loss 9.38488840726\n",
      "Itecr #: 6 loss 9.8641219436\n",
      "Itecr #: 7 loss 9.38807074629\n",
      "Itecr #: 8 loss 9.81562115335\n",
      "Itecr #: 9 loss 9.43808499171\n",
      "Itecr #: 10 loss 9.9482745964\n",
      "Itecr #: 11 loss 9.51271548951\n",
      "Itecr #: 12 loss 9.97095877957\n",
      "Itecr #: 13 loss 9.33314150213\n",
      "Itecr #: 14 loss 9.70517339076\n",
      "Itecr #: 15 loss 9.1011111511\n",
      "Itecr #: 16 loss 9.42437144481\n",
      "Itecr #: 17 loss 9.53643096015\n",
      "Itecr #: 18 loss 10.3767739669\n",
      "Itecr #: 19 loss 9.46319300524\n",
      "Itecr #: 20 loss 9.55336214024\n",
      "Itecr #: 21 loss 9.11218860316\n",
      "Itecr #: 22 loss 9.57168072733\n",
      "Itecr #: 23 loss 9.39077153572\n",
      "Itecr #: 24 loss 10.0093064257\n",
      "Itecr #: 25 loss 9.28570934782\n",
      "Itecr #: 26 loss 9.58303543613\n",
      "Itecr #: 27 loss 9.4999582266\n",
      "Itecr #: 28 loss 10.2637774015\n",
      "Itecr #: 29 loss 9.37109423071\n",
      "Itecr #: 30 loss 9.47533672502\n",
      "Itecr #: 31 loss 9.14071614627\n",
      "Itecr #: 32 loss 9.75932922934\n",
      "Itecr #: 33 loss 9.31123389191\n",
      "Itecr #: 34 loss 9.75989824573\n",
      "Itecr #: 35 loss 9.40369085649\n",
      "Itecr #: 36 loss 10.0581371453\n",
      "Itecr #: 37 loss 9.2671623117\n",
      "Itecr #: 38 loss 9.47288579091\n",
      "Itecr #: 39 loss 9.55120810134\n",
      "Itecr #: 40 loss 10.4725021854\n",
      "Itecr #: 41 loss 9.34442879384\n",
      "Itecr #: 42 loss 9.18909348736\n",
      "Itecr #: 43 loss 9.03208786952\n",
      "Itecr #: 44 loss 9.7157718441\n",
      "Itecr #: 45 loss 9.31383897741\n",
      "Itecr #: 46 loss 9.70858580318\n",
      "Itecr #: 47 loss 9.20729168019\n",
      "Itecr #: 48 loss 9.81096875404\n",
      "Itecr #: 49 loss 9.48205407276\n",
      "Itecr #: 50 loss 10.0695617715\n",
      "Itecr #: 51 loss 9.45307893904\n",
      "Itecr #: 52 loss 9.92555121233\n",
      "Itecr #: 53 loss 9.11060504966\n",
      "Itecr #: 54 loss 9.32867431401\n",
      "Itecr #: 55 loss 9.26849983885\n",
      "Itecr #: 56 loss 10.0888903662\n",
      "Itecr #: 57 loss 9.19797385738\n",
      "Itecr #: 58 loss 9.37569921306\n",
      "Itecr #: 59 loss 9.54507143688\n",
      "Itecr #: 60 loss 10.5769199442\n",
      "Itecr #: 61 loss 9.38085009053\n",
      "Itecr #: 62 loss 9.19846658154\n",
      "Itecr #: 63 loss 9.00314162926\n",
      "Itecr #: 64 loss 9.59770679501\n",
      "Itecr #: 65 loss 9.26981327559\n",
      "Itecr #: 66 loss 9.7610417752\n",
      "Itecr #: 67 loss 9.1474523087\n",
      "Itecr #: 68 loss 9.6796506008\n",
      "Itecr #: 69 loss 9.37126812642\n",
      "Itecr #: 70 loss 10.0676646431\n",
      "Itecr #: 71 loss 9.58470332988\n",
      "Itecr #: 72 loss 10.1944144368\n",
      "Itecr #: 73 loss 9.30608961335\n",
      "Itecr #: 74 loss 9.39139272631\n",
      "Itecr #: 75 loss 9.21841392854\n",
      "Itecr #: 76 loss 9.84390869305\n",
      "Itecr #: 77 loss 9.30562313484\n",
      "Itecr #: 78 loss 9.71018139413\n",
      "Itecr #: 79 loss 9.37656007552\n",
      "Itecr #: 80 loss 10.0759777348\n",
      "Itecr #: 81 loss 9.56901902771\n",
      "Itecr #: 82 loss 10.0513091906\n",
      "Itecr #: 83 loss 9.1756646582\n",
      "Itecr #: 84 loss 9.35729632919\n",
      "Itecr #: 85 loss 9.1245525939\n",
      "Itecr #: 86 loss 9.743998651\n",
      "Itecr #: 87 loss 9.27763518684\n",
      "Itecr #: 88 loss 9.79791636563\n",
      "Itecr #: 89 loss 9.34573847177\n",
      "Itecr #: 90 loss 9.98455865272\n",
      "Itecr #: 91 loss 9.35453632445\n",
      "Itecr #: 92 loss 9.79324769361\n",
      "Itecr #: 93 loss 9.31162113873\n",
      "Itecr #: 94 loss 9.9388100904\n",
      "Itecr #: 95 loss 9.41249644117\n",
      "Itecr #: 96 loss 10.002086111\n",
      "Itecr #: 97 loss 9.15074649053\n",
      "Itecr #: 98 loss 9.37530404201\n",
      "Itecr #: 99 loss 9.11895549584\n",
      "Itecr #: 100 loss 9.75544237506\n",
      "--- 26.956066846847534 seconds ---\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model_enwik.C = C_svd\n",
    "model_enwik.W = W_svd\n",
    "start_time = time.time()\n",
    "model_enwik.bfgd(d=200,from_iter=0, MAX_ITER=100, eta=8e-6, init=(True, C_svd, W_svd), display=True)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
