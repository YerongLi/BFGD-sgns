{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse.linalg import svds\n",
    "\n",
    "import itertools\n",
    "import pickle\n",
    "import math\n",
    "import re\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import word2vec, Word2Vec\n",
    "import logging\n",
    "\n",
    "from word2vec_as_MF import Word2vecMF\n",
    "from functions import *\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read and preprocess enwik9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 477 ms, sys: 84.2 ms, total: 561 ms\n",
      "Wall time: 1.38 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data = []\n",
    "with open('data/enwik8.txt') as file:\n",
    "# with open('data/xaa') as file:\n",
    "    for line in file:\n",
    "        data+= [line[:-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wiki_to_wordlist(sentence, remove_stopwords=False ):\n",
    "        # Function to convert a document to a sequence of words,\n",
    "    # optionally removing stop words.  Returns a list of words.\n",
    "\n",
    "    # 3. Convert words to lower case and split them\n",
    "    words = sentence.split()\n",
    "    #\n",
    "    # 4. Optionally remove stop words (false by default)\n",
    "    \n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w in stops]\n",
    "    #\n",
    "    # 5. Return a list of words\n",
    "    return(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing sentences from training set\n",
      "CPU times: user 2.7 s, sys: 256 ms, total: 2.95 s\n",
      "Wall time: 2.96 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# sentences = []  # Initialize an empty list of sentences\n",
    "\n",
    "print(\"Parsing sentences from training set\")\n",
    "\n",
    "    # sentences += [wiki_to_wordlist(sentence)]\n",
    "sentences = [sentence.split() for sentence in data]\n",
    "indices = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.07 s, sys: 112 ms, total: 1.18 s\n",
      "Wall time: 1.18 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for i, sentence in enumerate(sentences):\n",
    "    if not sentence:\n",
    "        pass\n",
    "    else:\n",
    "        indices.append(i)\n",
    "\n",
    "real_sentences = np.array(sentences)[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "real_sentences [ list(['however', 'these', 'tensions', 'are', 'less', 'prevalent', 'among', 'non', 'japanese', 'researchers'])\n",
      " list(['geographic', 'distribution'])\n",
      " list(['although', 'japanese', 'is', 'spoken', 'almost', 'exclusively', 'in', 'japan', 'it', 'has', 'been', 'and', 'is', 'still', 'sometimes', 'spoken', 'in', 'countries', 'besides', 'japan'])\n",
      " list(['when', 'japan', 'occupied', 'korea', 'taiwan', 'parts', 'of', 'china', 'and', 'various', 'pacific', 'islands', 'locals', 'in', 'those', 'countries', 'were', 'forced', 'to', 'learn', 'japanese', 'in', 'empire', 'building', 'programmes'])\n",
      " list(['as', 'a', 'result', 'there', 'are', 'still', 'many', 'people', 'in', 'these', 'countries', 'who', 'speak', 'japanese', 'instead', 'of', 'or', 'as', 'well', 'as', 'the', 'local', 'languages'])\n",
      " list(['in', 'addition', 'emigrants', 'from', 'japan', 'the', 'majority', 'of', 'whom', 'are', 'found', 'in', 'brazil', 'where', 'the', 'biggest', 'japanese', 'community', 'outside', 'japan', 'is', 'found', 'australia', 'especially', 'sydney', 'brisbane', 'and', 'melbourne', 'and', 'the', 'united', 'states', 'notably', 'california', 'and', 'hawaii', 'also', 'frequently', 'speak', 'japanese'])\n",
      " list(['there', 'is', 'also', 'a', 'small', 'community', 'in', 'davao', 'philippines'])\n",
      " list(['their', 'descendants', 'known', 'as', 'nikkei', 'literally', 'japanese', 'descendants', 'however', 'rarely', 'speak', 'japanese', 'fluently'])\n",
      " list(['there', 'are', 'estimated', 'to', 'be', 'several', 'million', 'non', 'japanese', 'studying', 'the', 'language', 'as', 'well'])\n",
      " list(['official', 'status'])\n",
      " list(['japanese', 'is', 'the', 'official', 'language', 'of', 'japan', 'and', 'japan', 'is', 'the', 'only', 'country', 'to', 'have', 'japanese', 'as', 'an', 'official', 'working', 'language'])\n",
      " list(['there', 'are', 'two', 'forms', 'of', 'the', 'language', 'considered', 'standard', 'or', 'standard', 'japanese', 'and', 'or', 'the', 'common', 'language'])\n",
      " list(['as', 'government', 'policy', 'has', 'modernized', 'japanese', 'many', 'of', 'the', 'distinctions', 'between', 'the', 'two', 'have', 'blurred'])\n",
      " list(['hy', 'jungo', 'is', 'taught', 'in', 'schools', 'and', 'used', 'on', 'television', 'and', 'in', 'official', 'communications', 'and', 'is', 'the', 'version', 'of', 'japanese', 'discussed', 'in', 'this', 'article'])\n",
      " list(['standard', 'japanese', 'can', 'also', 'be', 'divided', 'into', 'or', 'literary', 'language', 'and', 'or', 'oral', 'language', 'which', 'have', 'different', 'rules', 'of', 'grammar', 'and', 'some', 'variance', 'in', 'vocabulary'])\n",
      " list(['bungo', 'was', 'the', 'main', 'method', 'of', 'writing', 'japanese', 'until', 'the', 'late', 'one', 'nine', 'four', 'zero', 's', 'and', 'still', 'has', 'relevance', 'for', 'historians', 'literary', 'scholars', 'and', 'lawyers', 'many', 'japanese', 'laws', 'that', 'survived', 'world', 'war', 'ii', 'are', 'still', 'written', 'in', 'bungo', 'although', 'there', 'are', 'ongoing', 'efforts', 'to', 'modernize', 'their', 'language'])\n",
      " list(['k', 'go', 'is', 'the', 'predominant', 'method', 'of', 'speaking', 'and', 'writing', 'japanese', 'today', 'although', 'bungo', 'grammar', 'and', 'vocabulary', 'occasionally', 'appears', 'in', 'modern', 'japanese', 'for', 'poetic', 'effect'])\n",
      " list(['dialects'])\n",
      " list(['dozens', 'of', 'dialects', 'are', 'spoken', 'in', 'japan'])\n",
      " list(['the', 'profusion', 'is', 'due', 'to', 'the', 'mountainous', 'island', 'terrain', 'and', 'japan', 's', 'long', 'history', 'of', 'both', 'external', 'and', 'internal', 'isolation'])\n",
      " list(['dialects', 'typically', 'differ', 'in', 'terms', 'of', 'pitch', 'accent', 'inflectional', 'morphology', 'vocabulary', 'particle', 'usage', 'and', 'pronunciation'])\n",
      " list(['some', 'even', 'differ', 'in', 'vowel', 'and', 'consonant', 'inventories', 'although', 'this', 'is', 'uncommon'])\n",
      " list(['dialects', 'from', 'less', 'central', 'regions', 'such', 'as', 'the', 't', 'hoku', 'or', 'tsushima', 'dialect', 'may', 'be', 'unintelligible', 'to', 'speakers', 'from', 'other', 'parts', 'of', 'the', 'country'])\n",
      " list(['the', 'dialect', 'used', 'in', 'kagoshima', 'in', 'southern', 'ky', 'sh', 'is', 'famous', 'for', 'being', 'unintelligible', 'not', 'only', 'to', 'speakers', 'of', 'standard', 'japanese', 'but', 'to', 'speakers', 'of', 'nearby', 'dialects', 'elsewhere', 'in', 'ky', 'sh', 'as', 'well'])\n",
      " list(['kagoshima', 'dialect', 'is', 'eight', 'four', 'cognate', 'with', 'standard', 'tokyo', 'dialect'])\n",
      " list(['kansai', 'ben', 'a', 'group', 'of', 'dialects', 'from', 'west', 'central', 'japan', 'is', 'spoken', 'by', 'many', 'japanese', 'the', 'osaka', 'dialect', 'in', 'particular', 'is', 'associated', 'with', 'comedy', 'and', 'many', 'entertainers', 'use', 'osaka', 'dialect', 'phrases', 'solely', 'for', 'humor', 'value'])\n",
      " list(['the', 'ryukyuan', 'languages', 'are', 'spoken', 'in', 'the', 'ryukyu', 'islands'])\n",
      " list(['not', 'only', 'is', 'each', 'language', 'unintelligible', 'to', 'japanese', 'speakers', 'but', 'most', 'are', 'unintelligible', 'to', 'those', 'who', 'speak', 'other', 'ryukyuan', 'languages'])\n",
      " list(['due', 'to', 'the', 'close', 'relationship', 'of', 'ryukyuan', 'and', 'japanese', 'they', 'are', 'still', 'sometimes', 'said', 'to', 'be', 'only', 'dialects', 'of', 'one', 'language', 'but', 'modern', 'scholars', 'consider', 'them', 'to', 'be', 'separate', 'languages'])\n",
      " list(['recently', 'standard', 'japanese', 'ha'])]\n",
      "indices [903017, 903018, 903019, 903020, 903021, 903022, 903023, 903024, 903025, 903026, 903027, 903028, 903029, 903030, 903031, 903032, 903033, 903034, 903035, 903036, 903037, 903038, 903039, 903040, 903041, 903042, 903043, 903044, 903045, 903046]\n"
     ]
    }
   ],
   "source": [
    "print('real_sentences', real_sentences[-30:])\n",
    "print('indices', indices[-30:])\n",
    "# print('sentences', sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create word2vec as matrix factorization model\n",
    "model_enwik = Word2vecMF()\n",
    "model_enwik.data_to_matrices(real_sentences, 200, 5, 'enwik-200/matrices.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the model has been already created, load it from file\n",
    "model_enwik = Word2vecMF()\n",
    "model_enwik.load_matrices(from_file='enwik-200/matrices.npz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train ro_sgns model starting from SVD of SPPMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yerong/local/Anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:2: RuntimeWarning: divide by zero encountered in log\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# SVD initialization\n",
    "SPPMI = np.maximum(np.log(model_enwik.D) - np.log(model_enwik.B), 0)\n",
    "# print SPPMI\n",
    "u, s, vt = svds(SPPMI, k=200)\n",
    "C_svd = u.dot(np.sqrt(np.diag(s))).T\n",
    "W_svd = np.sqrt(np.diag(s)).dot(vt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_enwik.C = C_svd\n",
    "model_enwik.W = W_svd\n",
    "\n",
    "model_enwik.save_CW('enwik-200/initializations/SVD_dim200', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter #: 10 loss 126122526.847\n",
      "--- 260.27518939971924 seconds ---\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "start_time = time.time()\n",
    "opt_experiment(model_enwik,\n",
    "               mode='PS', \n",
    "               d=200,\n",
    "               eta = 5e-5,\n",
    "               MAX_ITER=10,\n",
    "               from_iter=0,\n",
    "               start_from='SVD',\n",
    "               init=(True, C_svd, W_svd), display=True)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Itecr #: 10 loss 987277110.005\n",
      "--- 31.619832277297974 seconds ---\n"
     ]
    }
   ],
   "source": [
    "model_enwik.C = C_svd\n",
    "model_enwik.W = W_svd\n",
    "start_time = time.time()\n",
    "model_enwik.bfgd(d=200,from_iter=0, MAX_ITER=10, eta=1e-6, init=(True, C_svd, W_svd), display=True)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 19s, sys: 1.34 s, total: 3min 20s\n",
      "Wall time: 1min 17s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = Word2Vec(real_sentences, size = 200, compute_loss=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23110102.0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_latest_training_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(real_sentences, size=200, window=5, min_count=5, workers=4)\n",
    "fname = 'original'\n",
    "model.save(fname)\n",
    "model = Word2Vec.load(fname)  # you can continue training with the loaded model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time print(dataPD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
