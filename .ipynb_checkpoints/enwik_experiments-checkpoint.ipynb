{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse.linalg import svds\n",
    "\n",
    "import itertools\n",
    "import pickle\n",
    "import math\n",
    "import re\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import word2vec, Word2Vec\n",
    "import logging\n",
    "\n",
    "from word2vec_as_MF import Word2vecMF\n",
    "from functions import *\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read and preprocess enwik9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "data = []\n",
    "with open('data/xaa') as file:\n",
    "# with open('data/xaa') as file:\n",
    "    for line in file:\n",
    "        data+= [line[:-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wiki_to_wordlist(sentence, remove_stopwords=False ):\n",
    "        # Function to convert a document to a sequence of words,\n",
    "    # optionally removing stop words.  Returns a list of words.\n",
    "\n",
    "    # 3. Convert words to lower case and split them\n",
    "    words = sentence.split()\n",
    "    #\n",
    "    # 4. Optionally remove stop words (false by default)\n",
    "    \n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w in stops]\n",
    "    #\n",
    "    # 5. Return a list of words\n",
    "    return(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing sentences from training set\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "# sentences = []  # Initialize an empty list of sentences\n",
    "\n",
    "print(\"Parsing sentences from training set\")\n",
    "\n",
    "# sentences += [wiki_to_wordlist(sentence)]\n",
    "sentences = [sentence.split() for sentence in data]\n",
    "indices = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "for i, sentence in enumerate(sentences):\n",
    "    if not sentence:\n",
    "        pass\n",
    "    else:\n",
    "        indices.append(i)\n",
    "\n",
    "real_sentences = np.array(sentences)[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"print('real_sentences', real_sentences[-30:])\\nprint('indices', indices[-30:])\\n# print('sentences', sentences)\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''print('real_sentences', real_sentences[-30:])\n",
    "print('indices', indices[-30:])\n",
    "# print('sentences', sentences)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create word2vec as matrix factorization model\n",
    "dimension = 100\n",
    "model_enwik = Word2vecMF()\n",
    "model_enwik.data_to_matrices(real_sentences, dimension, 5, 'enwik-200/matrices.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# If the model has been already created, load it from file\n",
    "model_enwik = Word2vecMF()\n",
    "model_enwik.load_matrices(from_file='enwik-200/matrices.npz')'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "skip = Word2Vec(real_sentences, size = dimension, compute_loss=True, min_count= 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skip.get_latest_training_loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train ro_sgns model starting from SVD of SPPMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yerong/local/Anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:2: RuntimeWarning: divide by zero encountered in log\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# SVD initialization\n",
    "SPPMI = np.maximum(np.log(model_enwik.D) - np.log(model_enwik.B), 0)\n",
    "# print SPPMI\n",
    "u, s, vt = svds(SPPMI, k=dimension)\n",
    "C_svd = u.dot(np.sqrt(np.diag(s))).T\n",
    "W_svd = np.sqrt(np.diag(s)).dot(vt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_enwik.C = C_svd\n",
    "model_enwik.W = W_svd\n",
    "\n",
    "model_enwik.save_CW('enwik-200/initializations/SVD_dim200', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''# Train the model\n",
    "start_time = time.time()\n",
    "opt_experiment(model_enwik,\n",
    "               mode='PS', \n",
    "               d=dimension,\n",
    "               eta = 5e-5,\n",
    "               MAX_ITER=10,\n",
    "               from_iter=0,\n",
    "               start_from='SVD',\n",
    "               init=(True, C_svd, W_svd), display=True)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enwik-200/AMiter_fromSVD_dim100_step5e-06_factors\n",
      "Iter #: 100 loss 1343887.05036\n",
      "Iter #: 200 loss 1167246.82651\n",
      "Iter #: 300 loss 1110686.97826\n",
      "Iter #: 400 loss 1083405.07033\n",
      "Iter #: 500 loss 1066944.73764\n",
      "Iter #: 600 loss 1055553.85028\n",
      "Iter #: 700 loss 1046963.87394\n",
      "Iter #: 800 loss 1040194.93718\n",
      "Iter #: 900 loss 1034739.53531\n",
      "Iter #: 1000 loss 1030265.23543\n",
      "Iter #: 1100 loss 1026531.4559\n",
      "Iter #: 1200 loss 1023360.87853\n",
      "Iter #: 1300 loss 1020624.39302\n",
      "Iter #: 1400 loss 1018229.47672\n",
      "Iter #: 1500 loss 1016109.61225\n",
      "Iter #: 1600 loss 1014215.90919\n",
      "Iter #: 1700 loss 1012511.34509\n",
      "Iter #: 1800 loss 1010967.05176\n",
      "Iter #: 1900 loss 1009559.95085\n",
      "Iter #: 2000 loss 1008271.22888\n",
      "Iter #: 2100 loss 1007085.32492\n",
      "Iter #: 2200 loss 1005989.23123\n",
      "Iter #: 2300 loss 1004971.98796\n",
      "Iter #: 2400 loss 1004024.3027\n",
      "Iter #: 2500 loss 1003138.2548\n",
      "Iter #: 2600 loss 1002307.06073\n",
      "Iter #: 2700 loss 1001524.88557\n",
      "Iter #: 2800 loss 1000786.69003\n",
      "Iter #: 2900 loss 1000088.1056\n",
      "Iter #: 3000 loss 999425.331697\n",
      "Iter #: 3100 loss 998795.050548\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "start_time = time.time()\n",
    "opt_experiment(model_enwik,\n",
    "               mode='AM', \n",
    "               d=dimension,\n",
    "               eta = 5e-6,\n",
    "               MAX_ITER=10000,\n",
    "               from_iter=0,\n",
    "               start_from='SVD',\n",
    "               init=(True, C_svd, W_svd), display=True)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_enwik.C = C_svd\n",
    "model_enwik.W = W_svd\n",
    "start_time = time.time()\n",
    "model_enwik.bfgd(d=dimension,from_iter=10000, MAX_ITER=10000, eta=5e-6, display=True,\n",
    "                 init=(True, C_svd, W_svd), \n",
    "                 save=[True, 'dataset'])\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''model = Word2Vec(real_sentences, size=200, window=5, min_count=5, workers=4)\n",
    "fname = 'original'\n",
    "model.save(fname)\n",
    "model = Word2Vec.load(fname)  # you can continue training with the loaded model!'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
