{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\stugov\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse.linalg import svds\n",
    "\n",
    "import itertools\n",
    "import pickle\n",
    "import math\n",
    "import re\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import word2vec, Word2Vec\n",
    "import logging\n",
    "\n",
    "from word2vec_as_MF import Word2vecMF\n",
    "from functions import *\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read and preprocess enwik9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "data = []\n",
    "with open('data/enwik8.txt') as file:\n",
    "# with open('data/xaa') as file:\n",
    "    for line in file:\n",
    "        data+= [line[:-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wiki_to_wordlist(sentence, remove_stopwords=False ):\n",
    "        # Function to convert a document to a sequence of words,\n",
    "    # optionally removing stop words.  Returns a list of words.\n",
    "\n",
    "    # 3. Convert words to lower case and split them\n",
    "    words = sentence.split()\n",
    "    #\n",
    "    # 4. Optionally remove stop words (false by default)\n",
    "    \n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w in stops]\n",
    "    #\n",
    "    # 5. Return a list of words\n",
    "    return(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing sentences from training set\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "# sentences = []  # Initialize an empty list of sentences\n",
    "\n",
    "print(\"Parsing sentences from training set\")\n",
    "\n",
    "# sentences += [wiki_to_wordlist(sentence)]\n",
    "sentences = [sentence.split() for sentence in data]\n",
    "indices = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "for i, sentence in enumerate(sentences):\n",
    "    if not sentence:\n",
    "        pass\n",
    "    else:\n",
    "        indices.append(i)\n",
    "\n",
    "real_sentences = np.array(sentences)[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"print('real_sentences', real_sentences[-30:])\\nprint('indices', indices[-30:])\\n# print('sentences', sentences)\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''print('real_sentences', real_sentences[-30:])\n",
    "print('indices', indices[-30:])\n",
    "# print('sentences', sentences)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# If the model has been already created, load it from file\n",
    "model_enwik = Word2vecMF()\n",
    "model_enwik.load_matrices(from_file='enwik-200/matrices.npz')'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dimension = 100\n",
    "skip = Word2Vec(real_sentences, size = dimension, compute_loss=True, iter=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33753356.0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skip.get_latest_training_loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train ro_sgns model starting from SVD of SPPMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create word2vec as matrix factorization model\n",
    "model_enwik = Word2vecMF()\n",
    "model_enwik.data_to_matrices(real_sentences, dimension, 5, 'enwik-200/matrices.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVD initialization\n",
    "SPPMI = np.maximum(np.log(model_enwik.D) - np.log(model_enwik.B), 0)\n",
    "# print SPPMI\n",
    "u, s, vt = svds(SPPMI, k=dimension)\n",
    "C_svd = u.dot(np.sqrt(np.diag(s))).T\n",
    "W_svd = np.sqrt(np.diag(s)).dot(vt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_enwik.C = C_svd\n",
    "model_enwik.W = W_svd\n",
    "\n",
    "model_enwik.save_CW('enwik-200/initializations/SVD_dim200', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''# Train the model\n",
    "start_time = time.time()\n",
    "opt_experiment(model_enwik,\n",
    "               mode='PS', \n",
    "               d=dimension,\n",
    "               eta = 5e-5,\n",
    "               MAX_ITER=10,\n",
    "               from_iter=0,\n",
    "               start_from='SVD',\n",
    "               init=(True, C_svd, W_svd), display=True)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enwik-200/AMiter_fromSVD_dim100_step1e-05_factors\n",
      "Iter #: 189100 loss 951553.4003238408\n",
      "Iter #: 189200 loss 951551.7481060625\n",
      "Iter #: 189300 loss 951550.0974237949\n",
      "Iter #: 189400 loss 951548.4482864127\n",
      "Iter #: 189500 loss 951546.8006916799\n",
      "Iter #: 189600 loss 951545.1546287638\n",
      "Iter #: 189700 loss 951543.5100875341\n",
      "Iter #: 189800 loss 951541.8670761357\n",
      "Iter #: 189900 loss 951540.225596485\n",
      "Iter #: 190000 loss 951538.5856364198\n",
      "Iter #: 190100 loss 951536.9472008224\n",
      "Iter #: 190200 loss 951535.3102881168\n",
      "Iter #: 190300 loss 951533.6748941138\n",
      "Iter #: 190400 loss 951532.0410162988\n",
      "Iter #: 190500 loss 951530.4086560642\n",
      "Iter #: 190600 loss 951528.7778006517\n",
      "Iter #: 190700 loss 951527.1484559232\n",
      "Iter #: 190800 loss 951525.5206200363\n",
      "Iter #: 190900 loss 951523.8942906026\n",
      "Iter #: 191000 loss 951522.2694653273\n",
      "Iter #: 191100 loss 951520.6461388695\n",
      "Iter #: 191200 loss 951519.0243138424\n",
      "Iter #: 191300 loss 951517.4039875346\n",
      "Iter #: 191400 loss 951515.7851537775\n",
      "Iter #: 191500 loss 951514.167809961\n",
      "Iter #: 191600 loss 951512.5519566952\n",
      "Iter #: 191700 loss 951510.9375974177\n",
      "Iter #: 191800 loss 951509.3247225677\n",
      "Iter #: 191900 loss 951507.7133285217\n",
      "Iter #: 192000 loss 951506.1034206258\n",
      "Iter #: 192100 loss 951504.4950015083\n",
      "Iter #: 192200 loss 951502.8880508691\n",
      "Iter #: 192300 loss 951501.2825768715\n",
      "Iter #: 192400 loss 951499.6785854205\n",
      "Iter #: 192500 loss 951498.0760594358\n",
      "Iter #: 192600 loss 951496.4750085695\n",
      "Iter #: 192700 loss 951494.875425251\n",
      "Iter #: 192800 loss 951493.2773057717\n",
      "Iter #: 192900 loss 951491.6806430209\n",
      "Iter #: 193000 loss 951490.0854429008\n",
      "Iter #: 193100 loss 951488.4917016351\n",
      "Iter #: 193200 loss 951486.8994243927\n",
      "Iter #: 193300 loss 951485.3085976273\n",
      "Iter #: 193400 loss 951483.7192314714\n",
      "Iter #: 193500 loss 951482.1313054978\n",
      "Iter #: 193600 loss 951480.5448312775\n",
      "Iter #: 193700 loss 951478.9598057785\n",
      "Iter #: 193800 loss 951477.3762246708\n",
      "Iter #: 193900 loss 951475.7940896568\n",
      "Iter #: 194000 loss 951474.2133947263\n",
      "Iter #: 194100 loss 951472.6341384327\n",
      "Iter #: 194200 loss 951471.0563234992\n",
      "Iter #: 194300 loss 951469.4799355248\n",
      "Iter #: 194400 loss 951467.9049842266\n",
      "Iter #: 194500 loss 951466.3314620107\n",
      "Iter #: 194600 loss 951464.7593686403\n",
      "Iter #: 194700 loss 951463.1887013894\n",
      "Iter #: 194800 loss 951461.6195093251\n",
      "Iter #: 194900 loss 951460.0516970062\n",
      "Iter #: 195000 loss 951458.485301256\n",
      "Iter #: 195100 loss 951456.9203271678\n",
      "Iter #: 195200 loss 951455.3567683415\n",
      "Iter #: 195300 loss 951453.7946264386\n",
      "Iter #: 195400 loss 951452.2338979215\n",
      "Iter #: 195500 loss 951450.6745837263\n",
      "Iter #: 195600 loss 951449.1166838175\n",
      "Iter #: 195700 loss 951447.560187898\n",
      "Iter #: 195800 loss 951446.0051021022\n",
      "Iter #: 195900 loss 951444.4514138588\n",
      "Iter #: 196000 loss 951442.8991253171\n",
      "Iter #: 196100 loss 951441.3482433006\n",
      "Iter #: 196200 loss 951439.7987524989\n",
      "Iter #: 196300 loss 951438.2506619496\n",
      "Iter #: 196400 loss 951436.7039702225\n",
      "Iter #: 196500 loss 951435.1586718957\n",
      "Iter #: 196600 loss 951433.6147586519\n",
      "Iter #: 196700 loss 951432.0722347908\n",
      "Iter #: 196800 loss 951430.5310916129\n",
      "Iter #: 196900 loss 951428.991341527\n",
      "Iter #: 197000 loss 951427.4529672285\n",
      "Iter #: 197100 loss 951425.9159799212\n",
      "Iter #: 197200 loss 951424.380371721\n",
      "Iter #: 197300 loss 951422.8461345319\n",
      "Iter #: 197400 loss 951421.3132764712\n",
      "Iter #: 197500 loss 951419.7817869303\n",
      "Iter #: 197600 loss 951418.2516770778\n",
      "Iter #: 197700 loss 951416.7229375587\n",
      "Iter #: 197800 loss 951415.1955631091\n",
      "Iter #: 197900 loss 951413.6695601955\n",
      "Iter #: 198000 loss 951412.1449137328\n",
      "Iter #: 198100 loss 951410.6216314356\n",
      "Iter #: 198200 loss 951409.0997035166\n",
      "Iter #: 198300 loss 951407.5791372592\n",
      "Iter #: 198400 loss 951406.0599367333\n",
      "Iter #: 198500 loss 951404.5420872404\n",
      "Iter #: 198600 loss 951403.025588139\n",
      "Iter #: 198700 loss 951401.510434326\n",
      "Iter #: 198800 loss 951399.9966307026\n",
      "Iter #: 198900 loss 951398.4841832303\n",
      "Iter #: 199000 loss 951396.9730717806\n",
      "Iter #: 199100 loss 951395.4633039499\n",
      "Iter #: 199200 loss 951393.9548829953\n",
      "Iter #: 199300 loss 951392.4478007747\n",
      "Iter #: 199400 loss 951390.9420577146\n",
      "Iter #: 199500 loss 951389.437653559\n",
      "Iter #: 199600 loss 951387.9345821558\n",
      "Iter #: 199700 loss 951386.432848996\n",
      "Iter #: 199800 loss 951384.9324410382\n",
      "Iter #: 199900 loss 951383.4333659664\n",
      "Iter #: 200000 loss 951381.9356234901\n",
      "Iter #: 200100 loss 951380.4392006014\n",
      "Iter #: 200200 loss 951378.9441067071\n",
      "Iter #: 200300 loss 951377.4503288635\n",
      "Iter #: 200400 loss 951375.9578674614\n",
      "Iter #: 200500 loss 951374.4667441493\n",
      "Iter #: 200600 loss 951372.9769249283\n",
      "Iter #: 200700 loss 951371.4884146367\n",
      "Iter #: 200800 loss 951370.0012222748\n",
      "Iter #: 200900 loss 951368.5153373862\n",
      "Iter #: 201000 loss 951367.030764186\n",
      "Iter #: 201100 loss 951365.5474983596\n",
      "Iter #: 201200 loss 951364.0655430986\n",
      "Iter #: 201300 loss 951362.5848935355\n",
      "Iter #: 201400 loss 951361.1055487411\n",
      "Iter #: 201500 loss 951359.6275038464\n",
      "Iter #: 201600 loss 951358.1507643398\n",
      "Iter #: 201700 loss 951356.6753150865\n",
      "Iter #: 201800 loss 951355.201159399\n",
      "Iter #: 201900 loss 951353.7282966679\n",
      "Iter #: 202000 loss 951352.2567293039\n",
      "Iter #: 202100 loss 951350.7864510034\n",
      "Iter #: 202200 loss 951349.317465865\n",
      "Iter #: 202300 loss 951347.8497715786\n",
      "Iter #: 202400 loss 951346.3833607543\n",
      "Iter #: 202500 loss 951344.9182335741\n",
      "Iter #: 202600 loss 951343.4543818391\n",
      "Iter #: 202700 loss 951341.9918118995\n",
      "Iter #: 202800 loss 951340.530521347\n",
      "Iter #: 202900 loss 951339.0705044882\n",
      "Iter #: 203000 loss 951337.6117659599\n",
      "Iter #: 203100 loss 951336.1543185097\n",
      "Iter #: 203200 loss 951334.6981321635\n",
      "Iter #: 203300 loss 951333.2432084983\n",
      "Iter #: 203400 loss 951331.7895534141\n",
      "Iter #: 203500 loss 951330.3371642967\n",
      "Iter #: 203600 loss 951328.8860456016\n",
      "Iter #: 203700 loss 951327.4361860333\n",
      "Iter #: 203800 loss 951325.9875888887\n",
      "Iter #: 203900 loss 951324.5402609792\n",
      "Iter #: 204000 loss 951323.0941852167\n",
      "Iter #: 204100 loss 951321.6493689724\n",
      "Iter #: 204200 loss 951320.2058110356\n",
      "Iter #: 204300 loss 951318.7634999593\n",
      "Iter #: 204400 loss 951317.3224428998\n",
      "Iter #: 204500 loss 951315.8826307623\n",
      "Iter #: 204600 loss 951314.444076701\n",
      "Iter #: 204700 loss 951313.0067657452\n",
      "Iter #: 204800 loss 951311.5706954141\n",
      "Iter #: 204900 loss 951310.1358775564\n",
      "Iter #: 205000 loss 951308.7022944984\n",
      "Iter #: 205100 loss 951307.2699535997\n",
      "Iter #: 205200 loss 951305.8388617606\n",
      "Iter #: 205300 loss 951304.408992231\n",
      "Iter #: 205400 loss 951302.9803640002\n",
      "Iter #: 205500 loss 951301.5529660255\n",
      "Iter #: 205600 loss 951300.1268050339\n",
      "Iter #: 205700 loss 951298.7018870325\n",
      "Iter #: 205800 loss 951297.2781868301\n",
      "Iter #: 205900 loss 951295.8557080394\n",
      "Iter #: 206000 loss 951294.4344621445\n",
      "Iter #: 206100 loss 951293.0144348191\n",
      "Iter #: 206200 loss 951291.5956294447\n",
      "Iter #: 206300 loss 951290.1780526295\n",
      "Iter #: 206400 loss 951288.761690097\n",
      "Iter #: 206500 loss 951287.346549298\n",
      "Iter #: 206600 loss 951285.9326191894\n",
      "Iter #: 206700 loss 951284.5199079299\n",
      "Iter #: 206800 loss 951283.1084119669\n",
      "Iter #: 206900 loss 951281.6981239504\n",
      "Iter #: 207000 loss 951280.2890527551\n",
      "Iter #: 207100 loss 951278.8811821705\n",
      "Iter #: 207200 loss 951277.4745253489\n",
      "Iter #: 207300 loss 951276.0690703606\n",
      "Iter #: 207400 loss 951274.6648152478\n",
      "Iter #: 207500 loss 951273.2617643978\n",
      "Iter #: 207600 loss 951271.8599149946\n",
      "Iter #: 207700 loss 951270.4592714969\n",
      "Iter #: 207800 loss 951269.0598241912\n",
      "Iter #: 207900 loss 951267.6615743034\n",
      "Iter #: 208000 loss 951266.2645131318\n",
      "Iter #: 208100 loss 951264.8686470358\n",
      "Iter #: 208200 loss 951263.4739744535\n",
      "Iter #: 208300 loss 951262.0804924702\n",
      "Iter #: 208400 loss 951260.6881907894\n",
      "Iter #: 208500 loss 951259.2970769171\n",
      "Iter #: 208600 loss 951257.9071530965\n",
      "Iter #: 208700 loss 951256.5184182764\n",
      "Iter #: 208800 loss 951255.130856018\n",
      "Iter #: 208900 loss 951253.7444828798\n",
      "Iter #: 209000 loss 951252.359291071\n",
      "Iter #: 209100 loss 951250.9752707047\n",
      "Iter #: 209200 loss 951249.592428361\n",
      "Iter #: 209300 loss 951248.2107703007\n",
      "Iter #: 209400 loss 951246.8302764458\n",
      "Iter #: 209500 loss 951245.4509464025\n",
      "Iter #: 209600 loss 951244.0727870965\n",
      "Iter #: 209700 loss 951242.695795473\n",
      "Iter #: 209800 loss 951241.3199808989\n",
      "Iter #: 209900 loss 951239.9453179082\n",
      "Iter #: 210000 loss 951238.5718252723\n",
      "Iter #: 210100 loss 951237.1995029411\n",
      "Iter #: 210200 loss 951235.8283346753\n",
      "Iter #: 210300 loss 951234.4583308785\n",
      "Iter #: 210400 loss 951233.089479554\n",
      "Iter #: 210500 loss 951231.7217849331\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter #: 210600 loss 951230.3552460615\n",
      "Iter #: 210700 loss 951228.9898606687\n",
      "Iter #: 210800 loss 951227.6256275373\n",
      "Iter #: 210900 loss 951226.2625942162\n",
      "Iter #: 211000 loss 951224.9006748004\n",
      "Iter #: 211100 loss 951223.5398847038\n",
      "Iter #: 211200 loss 951222.1802419003\n",
      "Iter #: 211300 loss 951220.8217633439\n",
      "Iter #: 211400 loss 951219.4644143797\n",
      "Iter #: 211500 loss 951218.1082057651\n",
      "Iter #: 211600 loss 951216.7531363654\n",
      "Iter #: 211700 loss 951215.3992091436\n",
      "Iter #: 211800 loss 951214.0464163532\n",
      "Iter #: 211900 loss 951212.6947731535\n",
      "Iter #: 212000 loss 951211.3442492802\n",
      "Iter #: 212100 loss 951209.9948752975\n",
      "Iter #: 212200 loss 951208.6466132558\n",
      "Iter #: 212300 loss 951207.2994922072\n",
      "Iter #: 212400 loss 951205.9535017783\n",
      "Iter #: 212500 loss 951204.6086395177\n",
      "Iter #: 212600 loss 951203.2649003362\n",
      "Iter #: 212700 loss 951201.9223153854\n",
      "Iter #: 212800 loss 951200.5808135653\n",
      "Iter #: 212900 loss 951199.2404431084\n",
      "Iter #: 213000 loss 951197.9011953619\n",
      "Iter #: 213100 loss 951196.5630548703\n",
      "Iter #: 213200 loss 951195.2260367805\n",
      "Iter #: 213300 loss 951193.8901357788\n",
      "Iter #: 213400 loss 951192.5553455785\n",
      "Iter #: 213500 loss 951191.2216723687\n",
      "Iter #: 213600 loss 951189.889119835\n",
      "Iter #: 213700 loss 951188.5576775109\n",
      "Iter #: 213800 loss 951187.2273306684\n",
      "Iter #: 213900 loss 951185.8980904302\n",
      "Iter #: 214000 loss 951184.5699585614\n",
      "Iter #: 214100 loss 951183.242931784\n",
      "Iter #: 214200 loss 951181.9170273957\n",
      "Iter #: 214300 loss 951180.5922067271\n",
      "Iter #: 214400 loss 951179.2684905446\n",
      "Iter #: 214500 loss 951177.9458745464\n",
      "Iter #: 214600 loss 951176.6243553343\n",
      "Iter #: 214700 loss 951175.3039253041\n",
      "Iter #: 214800 loss 951173.9845941619\n",
      "Iter #: 214900 loss 951172.6663685456\n",
      "Iter #: 215000 loss 951171.3492248286\n",
      "Iter #: 215100 loss 951170.0331666857\n",
      "Iter #: 215200 loss 951168.7182021277\n",
      "Iter #: 215300 loss 951167.4043263039\n",
      "Iter #: 215400 loss 951166.0915432309\n",
      "Iter #: 215500 loss 951164.7798420397\n",
      "Iter #: 215600 loss 951163.4692162088\n",
      "Iter #: 215700 loss 951162.1596797364\n",
      "Iter #: 215800 loss 951160.851223509\n",
      "Iter #: 215900 loss 951159.543844447\n",
      "Iter #: 216000 loss 951158.2375487909\n",
      "Iter #: 216100 loss 951156.9323382667\n",
      "Iter #: 216200 loss 951155.6282056589\n",
      "Iter #: 216300 loss 951154.3251360237\n",
      "Iter #: 216400 loss 951153.0231514792\n",
      "Iter #: 216500 loss 951151.7222242353\n",
      "Iter #: 216600 loss 951150.4223727856\n",
      "Iter #: 216700 loss 951149.1235866905\n",
      "Iter #: 216800 loss 951147.8258670005\n",
      "Iter #: 216900 loss 951146.5292189704\n",
      "Iter #: 217000 loss 951145.2336339022\n",
      "Iter #: 217100 loss 951143.9391104954\n",
      "Iter #: 217200 loss 951142.6456551696\n",
      "Iter #: 217300 loss 951141.3532601674\n",
      "Iter #: 217400 loss 951140.0619263535\n",
      "Iter #: 217500 loss 951138.7716442352\n",
      "Iter #: 217600 loss 951137.4824196922\n",
      "Iter #: 217700 loss 951136.1942623192\n",
      "Iter #: 217800 loss 951134.9071518965\n",
      "Iter #: 217900 loss 951133.6210978461\n",
      "Iter #: 218000 loss 951132.3360925072\n",
      "Iter #: 218100 loss 951131.0521451002\n",
      "Iter #: 218200 loss 951129.7692425156\n",
      "Iter #: 218300 loss 951128.4873974754\n",
      "Iter #: 218400 loss 951127.2065873995\n",
      "Iter #: 218500 loss 951125.9268233299\n",
      "Iter #: 218600 loss 951124.6481071056\n",
      "Iter #: 218700 loss 951123.370430871\n",
      "Iter #: 218800 loss 951122.0937937044\n",
      "Iter #: 218900 loss 951120.8181990089\n",
      "Iter #: 219000 loss 951119.5436403354\n",
      "Iter #: 219100 loss 951118.270131233\n",
      "Iter #: 219200 loss 951116.9976496324\n",
      "Iter #: 219300 loss 951115.7262023974\n",
      "Iter #: 219400 loss 951114.4557876058\n",
      "Iter #: 219500 loss 951113.1864191559\n",
      "Iter #: 219600 loss 951111.9180714283\n",
      "Iter #: 219700 loss 951110.6507530014\n",
      "Iter #: 219800 loss 951109.3844621982\n",
      "Iter #: 219900 loss 951108.11920204\n",
      "Iter #: 220000 loss 951106.8549672407\n",
      "Iter #: 220100 loss 951105.5917573784\n",
      "Iter #: 220200 loss 951104.3295772256\n",
      "Iter #: 220300 loss 951103.0684101023\n",
      "Iter #: 220400 loss 951101.8082706264\n",
      "Iter #: 220500 loss 951100.5491477641\n",
      "Iter #: 220600 loss 951099.2910517688\n",
      "Iter #: 220700 loss 951098.0339746107\n",
      "Iter #: 220800 loss 951096.777915457\n",
      "Iter #: 220900 loss 951095.5228633708\n",
      "Iter #: 221000 loss 951094.2688254052\n",
      "Iter #: 221100 loss 951093.0157991044\n",
      "Iter #: 221200 loss 951091.7637799992\n",
      "Iter #: 221300 loss 951090.5127782064\n",
      "Iter #: 221400 loss 951089.2627835405\n",
      "Iter #: 221500 loss 951088.0137898442\n",
      "Iter #: 221600 loss 951086.7658063375\n",
      "Iter #: 221700 loss 951085.5188342314\n",
      "Iter #: 221800 loss 951084.2728640704\n",
      "Iter #: 221900 loss 951083.0278893046\n",
      "Iter #: 222000 loss 951081.7839295688\n",
      "Iter #: 222100 loss 951080.5409603155\n",
      "Iter #: 222200 loss 951079.2989897862\n",
      "Iter #: 222300 loss 951078.058019915\n",
      "Iter #: 222400 loss 951076.8180437576\n",
      "Iter #: 222500 loss 951075.5790630984\n",
      "Iter #: 222600 loss 951074.3410725573\n",
      "Iter #: 222700 loss 951073.1040917563\n",
      "Iter #: 222800 loss 951071.8681114095\n",
      "Iter #: 222900 loss 951070.6331005247\n",
      "Iter #: 223000 loss 951069.3990848414\n",
      "Iter #: 223100 loss 951068.166052333\n",
      "Iter #: 223200 loss 951066.9340079314\n",
      "Iter #: 223300 loss 951065.7029538022\n",
      "Iter #: 223400 loss 951064.4728812947\n",
      "Iter #: 223500 loss 951063.24379202\n",
      "Iter #: 223600 loss 951062.015682261\n",
      "Iter #: 223700 loss 951060.7885620748\n",
      "Iter #: 223800 loss 951059.5624155172\n",
      "Iter #: 223900 loss 951058.3372512729\n",
      "Iter #: 224000 loss 951057.1130629394\n",
      "Iter #: 224100 loss 951055.8898536843\n",
      "Iter #: 224200 loss 951054.667609494\n",
      "Iter #: 224300 loss 951053.4463395885\n",
      "Iter #: 224400 loss 951052.2260464655\n",
      "Iter #: 224500 loss 951051.0067224592\n",
      "Iter #: 224600 loss 951049.7883651244\n",
      "Iter #: 224700 loss 951048.5709820539\n",
      "Iter #: 224800 loss 951047.3545643117\n",
      "Iter #: 224900 loss 951046.1391216295\n",
      "Iter #: 225000 loss 951044.924657184\n",
      "Iter #: 225100 loss 951043.711134397\n",
      "Iter #: 225200 loss 951042.4985802682\n",
      "Iter #: 225300 loss 951041.286990746\n",
      "Iter #: 225400 loss 951040.0763540264\n",
      "Iter #: 225500 loss 951038.8666792763\n",
      "Iter #: 225600 loss 951037.657969464\n",
      "Iter #: 225700 loss 951036.4502086007\n",
      "Iter #: 225800 loss 951035.2434103631\n",
      "Iter #: 225900 loss 951034.0376371489\n",
      "Iter #: 226000 loss 951032.8327459821\n",
      "Iter #: 226100 loss 951031.6288071489\n",
      "Iter #: 226200 loss 951030.4258205242\n",
      "Iter #: 226300 loss 951029.2237817169\n",
      "Iter #: 226400 loss 951028.022690995\n",
      "Iter #: 226500 loss 951026.8225499955\n",
      "Iter #: 226600 loss 951025.6233649341\n",
      "Iter #: 226700 loss 951024.4251168926\n",
      "Iter #: 226800 loss 951023.2278149675\n",
      "Iter #: 226900 loss 951022.0314577882\n",
      "Iter #: 227000 loss 951020.8360460164\n",
      "Iter #: 227100 loss 951019.6415788553\n",
      "Iter #: 227200 loss 951018.4480474298\n",
      "Iter #: 227300 loss 951017.2554722087\n",
      "Iter #: 227400 loss 951016.0638136602\n",
      "Iter #: 227500 loss 951014.873095173\n",
      "Iter #: 227600 loss 951013.6833085408\n",
      "Iter #: 227700 loss 951012.4944719097\n",
      "Iter #: 227800 loss 951011.306555468\n",
      "Iter #: 227900 loss 951010.1195868406\n",
      "Iter #: 228000 loss 951008.9335351883\n",
      "Iter #: 228100 loss 951007.7484155465\n",
      "Iter #: 228200 loss 951006.564231491\n",
      "Iter #: 228300 loss 951005.3809764718\n",
      "Iter #: 228400 loss 951004.1986465353\n",
      "Iter #: 228500 loss 951003.0172345999\n",
      "Iter #: 228600 loss 951001.8367512766\n",
      "Iter #: 228700 loss 951000.6571859241\n",
      "Iter #: 228800 loss 950999.4785528259\n",
      "Iter #: 228900 loss 950998.3008364016\n",
      "Iter #: 229000 loss 950997.1240384383\n",
      "Iter #: 229100 loss 950995.9481612494\n",
      "Iter #: 229200 loss 950994.7732014857\n",
      "Iter #: 229300 loss 950993.599160644\n",
      "Iter #: 229400 loss 950992.4260384173\n",
      "Iter #: 229500 loss 950991.2538311782\n",
      "Iter #: 229600 loss 950990.0825317423\n",
      "Iter #: 229700 loss 950988.9121461443\n",
      "Iter #: 229800 loss 950987.7426808719\n",
      "Iter #: 229900 loss 950986.5741249864\n",
      "Iter #: 230000 loss 950985.4064769452\n",
      "Iter #: 230100 loss 950984.2397270598\n",
      "Iter #: 230200 loss 950983.0739030634\n",
      "Iter #: 230300 loss 950981.9089775014\n",
      "Iter #: 230400 loss 950980.7449578865\n",
      "Iter #: 230500 loss 950979.5818449286\n",
      "Iter #: 230600 loss 950978.4196246924\n",
      "Iter #: 230700 loss 950977.2583081169\n",
      "Iter #: 230800 loss 950976.0978916892\n",
      "Iter #: 230900 loss 950974.9383737035\n",
      "Iter #: 231000 loss 950973.7797585689\n",
      "Iter #: 231100 loss 950972.6220399096\n",
      "Iter #: 231200 loss 950971.46521937\n",
      "Iter #: 231300 loss 950970.3092909539\n",
      "Iter #: 231400 loss 950969.1542532851\n",
      "Iter #: 231500 loss 950968.0001133895\n",
      "Iter #: 231600 loss 950966.8468693382\n",
      "Iter #: 231700 loss 950965.6945226575\n",
      "Iter #: 231800 loss 950964.5430612126\n",
      "Iter #: 231900 loss 950963.3924843814\n",
      "Iter #: 232000 loss 950962.2427993583\n",
      "Iter #: 232100 loss 950961.0939933343\n",
      "Iter #: 232200 loss 950959.9460755144\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter #: 232300 loss 950958.7990512041\n",
      "Iter #: 232400 loss 950957.652920941\n",
      "Iter #: 232500 loss 950956.507662437\n",
      "Iter #: 232600 loss 950955.3632807444\n",
      "Iter #: 232700 loss 950954.2197753437\n",
      "Iter #: 232800 loss 950953.0771559777\n",
      "Iter #: 232900 loss 950951.93541652\n",
      "Iter #: 233000 loss 950950.7945588115\n",
      "Iter #: 233100 loss 950949.6545734599\n",
      "Iter #: 233200 loss 950948.5154709017\n",
      "Iter #: 233300 loss 950947.3772414167\n",
      "Iter #: 233400 loss 950946.2398760394\n",
      "Iter #: 233500 loss 950945.1033894998\n",
      "Iter #: 233600 loss 950943.9677688418\n",
      "Iter #: 233700 loss 950942.8330261795\n",
      "Iter #: 233800 loss 950941.6991992374\n",
      "Iter #: 233900 loss 950940.5661861696\n",
      "Iter #: 234000 loss 950939.4340456685\n",
      "Iter #: 234100 loss 950938.3027716946\n",
      "Iter #: 234200 loss 950937.1723624398\n",
      "Iter #: 234300 loss 950936.0428191305\n",
      "Iter #: 234400 loss 950934.9141346315\n",
      "Iter #: 234500 loss 950933.7863132596\n",
      "Iter #: 234600 loss 950932.6593595817\n",
      "Iter #: 234700 loss 950931.5332679254\n",
      "Iter #: 234800 loss 950930.4080267773\n",
      "Iter #: 234900 loss 950929.2836451645\n",
      "Iter #: 235000 loss 950928.1601258856\n",
      "Iter #: 235100 loss 950927.0374588429\n",
      "Iter #: 235200 loss 950925.9156492364\n",
      "Iter #: 235300 loss 950924.7947014002\n",
      "Iter #: 235400 loss 950923.6746042325\n",
      "Iter #: 235500 loss 950922.5553585397\n",
      "Iter #: 235600 loss 950921.4369650853\n",
      "Iter #: 235700 loss 950920.3194147004\n",
      "Iter #: 235800 loss 950919.2027188095\n",
      "Iter #: 235900 loss 950918.0868749574\n",
      "Iter #: 236000 loss 950916.9718691107\n",
      "Iter #: 236100 loss 950915.8577157895\n",
      "Iter #: 236200 loss 950914.7444074929\n",
      "Iter #: 236300 loss 950913.6319438694\n",
      "Iter #: 236400 loss 950912.5203268386\n",
      "Iter #: 236500 loss 950911.409551212\n",
      "Iter #: 236600 loss 950910.299613346\n",
      "Iter #: 236700 loss 950909.1905268471\n",
      "Iter #: 236800 loss 950908.0822680444\n",
      "Iter #: 236900 loss 950906.9748529814\n",
      "Iter #: 237000 loss 950905.8682730006\n",
      "Iter #: 237100 loss 950904.7625377644\n",
      "Iter #: 237200 loss 950903.6576336555\n",
      "Iter #: 237300 loss 950902.5535695965\n",
      "Iter #: 237400 loss 950901.4503388284\n",
      "Iter #: 237500 loss 950900.3479313528\n",
      "Iter #: 237600 loss 950899.2463612601\n",
      "Iter #: 237700 loss 950898.1456343979\n",
      "Iter #: 237800 loss 950897.0457301696\n",
      "Iter #: 237900 loss 950895.946652615\n",
      "Iter #: 238000 loss 950894.8483958255\n",
      "Iter #: 238100 loss 950893.7509678322\n",
      "Iter #: 238200 loss 950892.6543650351\n",
      "Iter #: 238300 loss 950891.5586071567\n",
      "Iter #: 238400 loss 950890.4636540753\n",
      "Iter #: 238500 loss 950889.3695294072\n",
      "Iter #: 238600 loss 950888.2762317525\n",
      "Iter #: 238700 loss 950887.1837551469\n",
      "Iter #: 238800 loss 950886.0920885435\n",
      "Iter #: 238900 loss 950885.0012409609\n",
      "Iter #: 239000 loss 950883.9112154882\n",
      "--- 614.1901476383209 seconds ---\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "start_time = time.time()\n",
    "opt_experiment(model_enwik,\n",
    "               mode='AM', \n",
    "               d=dimension,\n",
    "               eta = 1e-5,\n",
    "               MAX_ITER=50000,\n",
    "               from_iter=189000,\n",
    "               start_from='SVD',\n",
    "               init=(True, None, None), display=True)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_enwik.C = C_svd\n",
    "model_enwik.W = W_svd\n",
    "start_time = time.time()\n",
    "model_enwik.bfgd(d=dimension,from_iter=10000, MAX_ITER=10000, eta=5e-6, display=True,\n",
    "                 init=(True, C_svd, W_svd), \n",
    "                 save=[True, 'dataset'])\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''model = Word2Vec(real_sentences, size=200, window=5, min_count=5, workers=4)\n",
    "fname = 'original'\n",
    "model.save(fname)\n",
    "model = Word2Vec.load(fname)  # you can continue training with the loaded model!'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
